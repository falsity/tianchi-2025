"""è°ƒç”¨ç‹¬å æ—¶é—´ç®—å­ï¼Œåˆ†ætraceçš„ç‹¬å æ—¶é—´æ˜¯åœ¨ä»€ä¹ˆåœ°æ–¹å¢åŠ çš„ã€‚å¯¹äºå¯¹ç‹¬å æ—¶é—´è´¡çŒ®å‰95%çš„spanï¼Œæˆ‘ä»¬æ‰¾å‡ºæ¥ã€‚

æ–°å¢åŠŸèƒ½ï¼š
1. æ”¯æŒè®¡ç®—normalæ—¶é—´æ®µå†…æ¯ä¸ªserviceNameçš„å¹³å‡ç‹¬å æ—¶é—´
2. æ”¯æŒä»å¼‚å¸¸æ—¶é—´æ®µçš„ç‹¬å æ—¶é—´ä¸­å‡å»å¯¹åº”serviceNameçš„å¹³å‡å€¼(å½“minus_average=Trueæ—¶)
3. è¿™æ ·å¯ä»¥è¯†åˆ«å‡ºç›¸å¯¹äºæ­£å¸¸æƒ…å†µï¼Œç‹¬å æ—¶é—´å¼‚å¸¸å¢åŠ çš„span
4. æ”¯æŒåªå¤„ç†æ¯ä¸ªtraceä¸­ç‹¬å æ—¶é—´æ’top-1çš„span(å½“only_top1_per_trace=Trueæ—¶)
   - æ˜¾è‘—å‡å°‘è®¡ç®—é‡ï¼Œèšç„¦äºæ¯ä¸ªtraceçš„æœ€è€—æ—¶æ“ä½œ
   - é€‚ç”¨äºå¿«é€Ÿå®šä½æ¯ä¸ªtraceçš„ä¸»è¦ç“¶é¢ˆç‚¹
   - é‡è¦ï¼šå½“minus_average=Trueæ—¶ï¼Œä¼šå…ˆå¯¹æ‰€æœ‰spanå‡å»å¹³å‡å€¼ï¼Œå†é€‰æ‹©è°ƒæ•´åæ—¶é—´æœ€é•¿çš„span

æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆï¼š
æ–¹æ¡ˆ1ï¼ˆæ¨èï¼‰ï¼šç›´æ¥ä»span_listè·å–serviceNameå’ŒspanName
- ä¿®æ”¹SPLæŸ¥è¯¢ï¼ŒåŒæ—¶è¿”å›trace_id, span_list, span_id, span_index, exclusive_duration
- é€šè¿‡span_indexç›´æ¥æ˜ å°„åˆ°span_listä¸­çš„servicenameå’Œspanname
- å®Œå…¨é¿å…é‡æ–°æŸ¥è¯¢spansæ•°æ®ï¼Œæ€§èƒ½æœ€ä¼˜
- ä½¿ç”¨_adjust_durations_directlyå‡½æ•°è¿›è¡Œæœ¬åœ°è®¡ç®—

æ–¹æ¡ˆ2ï¼ˆå¤‡é€‰ï¼‰ï¼šé‡‡æ ·ä¼˜åŒ–
- å½“æ–¹æ¡ˆ1ä¸å¯ç”¨æ—¶çš„å¤‡é€‰æ–¹æ¡ˆ
- å¯¹spanè¿›è¡Œé‡‡æ ·ï¼Œæœ€å¤šå¤„ç†HIGH_RT_TRACESä¸ªspan
- å‡å°‘æŸ¥è¯¢è´Ÿè½½ï¼Œä½†ä»éœ€è¦é¢å¤–çš„æ•°æ®åº“æŸ¥è¯¢
- ä½¿ç”¨_adjust_durations_with_span_averageå‡½æ•°ï¼Œå†…ç½®é‡‡æ ·æœºåˆ¶
"""

import numpy as np
from utils.constants import HIGH_RT_TRACES, TRACES_FOR_AVG_RT, PERCENT_95, MAX_DURATION
import os
import logging

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# ç¯å¢ƒå˜é‡é…ç½®
from aliyun.log import LogClient, GetLogsRequest


# è°ƒç”¨è‡ªå®šä¹‰å‡½æ•° trace_exclusive_duration è®¡ç®—æ¯ä¸ª span çš„ç‹¬å æ—¶é—´
def get_spl_query(duration_threshold: int, limit_num: int) -> str:
    # print (f"duration_threshold: {duration_threshold}, limit_num: {limit_num}")
    spl = f"""* and duration > {duration_threshold} | set session mode=scan;  set session velox_use_io_executor=true; set session presto_velox_mix_run_not_check_linked_agg_enabled=true;
set session presto_velox_mix_run_support_complex_type_enabled=true;
set session velox_sanity_limit_enabled=false;
set session enable_remote_functions=true;
with t1 as (
    select traceId as trace_id,
        array_agg(
            cast(
                row(
                    case
                        when spanId is null then ''
                        else spanId
                    end,
                    case
                        when parentSpanId is null then ''
                        else parentSpanId
                    end,
                    case
                        when statusCode is null then ''
                        else cast(statusCode as varchar)
                    end,
                    case
                        when serviceName is null then ''
                        else serviceName
                    end,
                    case
                        when spanName is null then ''
                        else spanName
                    end,
                    case
                        when hostname is null then ''
                        else hostname
                    end,
                    case
                        when kind is null then ''
                        else kind
                    end,
                    case
                        when duration is null then -1
                        else cast(duration as bigint)
                    end,
                    case
                        when startTime is null then -1
                        else cast(startTime as bigint)
                    end,
                    case
                        when endTime is null then -1
                        else cast(endTime as bigint)
                    end,
                    case
                        when resources is null then ''
                        else resources
                    end,
                    case
                        when attributes is null then ''
                        else attributes
                    end,
                    case
                        when links is null then ''
                        else links
                    end,
                    case
                        when events is null then ''
                        else events
                    end,
                    case
                        when pid is null then '{{}}'          -- extraInfo
                        else concat('{{"pid":"', pid, '"}}')
                    end
                ) as row (
                    spanid varchar,
                    parentspanid varchar,
                    statuscode varchar,
                    servicename varchar,
                    spanname varchar,
                    hostname varchar,
                    kind varchar,
                    duration bigint,
                    starttime bigint,
                    endtime bigint,
                    resources varchar,
                    attributes varchar,
                    links varchar,
                    events varchar,
                    extrainfo varchar
                )
            )
        ) as span_list
    from log
    where (
            traceId is not null
            and traceId != ''
        )
        and (
            spanId is not null
            and spanId != ''
        )
        and (parentSpanId is not null)
        and (startTime is not null)
        and (duration is not null or endTime is not null)
    group by traceId
    order by trace_id limit {limit_num}
)
select
    trace_id,
    span_list,
    trace_exclusive_duration_result.span_id as span_id,
    trace_exclusive_duration_result.span_index,
    trace_exclusive_duration_result.exclusive_duration
from (
    select trace_id, span_list, trace_exclusive_duration(trace_id, span_list) as trace_exclusive_duration_result
    from (
        select * from t1 order by trace_id limit {limit_num}
    )
)
order by span_id limit {limit_num}
"""
    return spl


# ä¸Šé¢è¿™ä¸ªSPLè¯­å¥æ‰§è¡Œå®Œæˆåï¼Œä¼šå¾—åˆ°ç±»ä¼¼ä¸‹é¢çš„ç»“æœï¼šæ¯ä¸ªtrace_idå¯¹åº”ä¸€è¡Œã€‚æœ‰ä¸‰åˆ— span_idã€span_indexã€exclusive_duration, æ¯ä¸€åˆ—éƒ½æ˜¯arrayç±»å‹ï¼Œé•¿åº¦ç›¸ç­‰ã€‚
# æˆ‘ä»¬éœ€è¦å°†æ‰€æœ‰span_idå¯¹åº”çš„exclusive_durationæ‰¾å‡ºæ¥ï¼Œæ’åºï¼Œç„¶åæŠŠ exclusive_duration å å‰95% çš„span_idæ‰¾å‡ºæ¥
"""
span_id,    span_index,    exclusive_duration
["47536226199e3d1a","251e1d6125734573","ce05f5dcd45af66d"],    [1,2,3],    [580000,50000,6679751]
["a7ae3f4e057518c9","2c6c77123d1d76df","9951956134123686","2b3f640090a8c85d","be5dc34095786ad0","5d2b07e043008454"],    [1,2,3,4,5,6],     [919000,198000,761127,1954869,530163,1000000]
"""


class FindRootCauseSpansRT:
    def __init__(
        self,
        client,
        project_name: str,
        logstore_name: str,
        region: str,
        start_time: str,
        end_time: str,
        duration_threshold: int = 1000000,
        limit_num: int = 1000,
        normal_start_time: str = None,  # type: ignore
        normal_end_time: str = None,
        minus_average: bool = False,
        only_top1_per_trace: bool = False,
    ):  # type: ignore
        """
        åˆå§‹åŒ–FindRootCauseSpansRTç±»

        Args:
            client: SLSå®¢æˆ·ç«¯
            project_name: SLSé¡¹ç›®åç§°
            logstore_name: æ—¥å¿—åº“åç§°
            region: åœ°åŸŸ
            start_time: å¼‚å¸¸å¼€å§‹æ—¶é—´
            end_time: å¼‚å¸¸ç»“æŸæ—¶é—´
            duration_threshold: æŒç»­æ—¶é—´é˜ˆå€¼ï¼Œé»˜è®¤1000000ï¼ˆ1ç§’ï¼‰
            limit_num: é™åˆ¶å¤„ç†çš„traceæ•°é‡ï¼Œé»˜è®¤100
            normal_start_time: æ­£å¸¸æ—¶é—´æ®µå¼€å§‹æ—¶é—´
            normal_end_time: æ­£å¸¸æ—¶é—´æ®µç»“æŸæ—¶é—´
            minus_average: æ˜¯å¦å‡å»å¹³å‡å€¼ï¼Œé»˜è®¤False
            only_top1_per_trace: æ˜¯å¦åªå¤„ç†æ¯ä¸ªtraceä¸­ç‹¬å æ—¶é—´æ’top-1çš„spanï¼Œé»˜è®¤False
        """
        self.client = client
        self.project_name = project_name
        self.logstore_name = logstore_name
        self.region = region
        self.start_time = start_time
        self.end_time = end_time
        self.duration_threshold = duration_threshold
        self.limit_num = limit_num
        self.normal_start_time = normal_start_time
        self.normal_end_time = normal_end_time
        self.minus_average = minus_average
        self.only_top1_per_trace = only_top1_per_trace

        # å­˜å‚¨æ¯ä¸ªspanNameçš„å¹³å‡ç‹¬å æ—¶é—´
        self.span_average_durations = {}

        # å¦‚æœéœ€è¦å‡å»å¹³å‡å€¼ï¼Œå…ˆè®¡ç®—æ­£å¸¸æ—¶é—´æ®µçš„å¹³å‡å€¼
        if self.minus_average and self.normal_start_time and self.normal_end_time:
            self._calculate_span_averages()

    def find_top_95_percent_spans(self) -> list[str]:
        """
        æŸ¥æ‰¾å¼‚å¸¸æ—¶é—´æ®µå†…å å‰95%ç‹¬å æ—¶é—´çš„span_idåˆ—è¡¨

        Returns:
            span_idåˆ—è¡¨
        """
        # æ‰§è¡ŒSPLæŸ¥è¯¢è·å–ç‹¬å æ—¶é—´æ•°æ®
        query = get_spl_query(self.duration_threshold, HIGH_RT_TRACES)

        request = GetLogsRequest(
            project=self.project_name,
            logstore=self.logstore_name,
            query=query,
            fromTime=self.start_time,
            toTime=self.end_time,
            # line=HIGH_RT_TRACES # only calculate for 1000 traces
        )

        try:
            response = self.client.get_logs(request)
            logs = (
                [log_item.get_contents() for log_item in response.get_logs()]
                if response
                else []
            )
            logger.info("æŸ¥è¯¢åˆ°çš„æ—¥å¿—æ¡æ•°: %d", len(logs))

            return self._process_exclusive_duration_data(logs)

        except Exception as e:
            logger.error("æŸ¥è¯¢SLSæ—¶å‘ç”Ÿé”™è¯¯: %s", e)
            return []

    def _process_exclusive_duration_data(self, logs: list) -> list[str]:
        """
        å¤„ç†ç‹¬å æ—¶é—´æ•°æ®ï¼Œæ‰¾å‡ºå å‰95%çš„span_id

        Args:
            logs: SLSæŸ¥è¯¢è¿”å›çš„æ—¥å¿—æ•°æ®

        Returns:
            å å‰95%ç‹¬å æ—¶é—´çš„span_idåˆ—è¡¨
        """
        # è¾“å‡ºå¤„ç†æ¨¡å¼ï¼Œé»˜è®¤é€‰æ‹©å¤„ç†æ‰€æœ‰span
        if self.only_top1_per_trace:
            if self.minus_average and self.span_average_durations:
                mode_description = (
                    "åªå¤„ç†æ¯ä¸ªtraceä¸­è°ƒæ•´åç‹¬å æ—¶é—´æ’top-1çš„spanï¼ˆå…ˆå‡å»å¹³å‡å€¼å†é€‰æ‹©ï¼‰"
                )
            else:
                mode_description = "åªå¤„ç†æ¯ä¸ªtraceä¸­åŸå§‹ç‹¬å æ—¶é—´æ’top-1çš„span"
        else:
            mode_description = "å¤„ç†æ¯ä¸ªtraceä¸­çš„æ‰€æœ‰span"
        logger.info("ğŸ”§ å¤„ç†æ¨¡å¼: %s", mode_description)

        # æ”¶é›†æ‰€æœ‰çš„span_idå’Œå¯¹åº”çš„ç‹¬å æ—¶é—´
        span_duration_mapping = {}  # {span_id: exclusive_duration}
        span_service_mapping = {}  # {span_id: (serviceName, spanName)} - æ–°å¢ï¼šç›´æ¥ä»span_listè·å–

        for log in logs:
            try:
                # è§£ææ•°ç»„å­—æ®µ
                span_ids = self._parse_array_field(log.get("span_id", "[]"))
                span_indices = self._parse_array_field(log.get("span_index", "[]"))
                exclusive_durations = self._parse_array_field(
                    log.get("exclusive_duration", "[]")
                )
                span_list = self._parse_array_field(log.get("span_list", "[]"))

                # ç¡®ä¿æ•°ç»„é•¿åº¦ä¸€è‡´
                if len(span_ids) != len(exclusive_durations) or len(span_ids) != len(
                    span_indices
                ):
                    logger.warning(
                        "æ•°ç»„é•¿åº¦ä¸ä¸€è‡´ - span_id(%d), span_index(%d), exclusive_duration(%d)",
                        len(span_ids),
                        len(span_indices),
                        len(exclusive_durations),
                    )
                    continue

                # æ”¶é›†span_idå’Œå¯¹åº”çš„exclusive_durationï¼ŒåŒæ—¶ä»span_listè·å–serviceNameå’ŒspanName
                if self.only_top1_per_trace:
                    # åªå¤„ç†æ¯ä¸ªtraceä¸­ç‹¬å æ—¶é—´æ’top-1çš„span
                    if self.minus_average and self.span_average_durations:
                        # éœ€è¦å‡å»å¹³å‡å€¼çš„æƒ…å†µï¼šå…ˆè®¡ç®—æ‰€æœ‰spançš„è°ƒæ•´åæ—¶é—´ï¼Œå†é€‰æ‹©top-1
                        adjusted_spans = []
                        for span_id, span_index, duration in zip(
                            span_ids, span_indices, exclusive_durations
                        ):
                            if isinstance(duration, (int, float)) and duration > 0:
                                # æˆªæ–­å¼‚å¸¸é•¿çš„durationï¼Œå¤„ç†outliers
                                truncated_duration = min(duration, MAX_DURATION)

                                # è·å–serviceNameå’ŒspanNameï¼Œè®¡ç®—è°ƒæ•´åçš„æ—¶é—´
                                adjusted_duration = truncated_duration
                                if span_list and 0 <= span_index < len(span_list):
                                    span_info = span_list[span_index]
                                    service_name, span_name = (
                                        self._extract_service_and_span_name(span_info)
                                    )
                                    if service_name and span_name:
                                        combined_key = f"{service_name}<sep>{span_name}"
                                        if combined_key in self.span_average_durations:
                                            avg_duration = self.span_average_durations[
                                                combined_key
                                            ]
                                            adjusted_duration = max(
                                                0, truncated_duration - avg_duration
                                            )
                                            adjusted_duration = min(
                                                adjusted_duration, MAX_DURATION
                                            )

                                adjusted_spans.append(
                                    (
                                        span_id,
                                        span_index,
                                        adjusted_duration,
                                        truncated_duration,
                                    )
                                )

                        # æ‰¾åˆ°è°ƒæ•´åç‹¬å æ—¶é—´æœ€é•¿çš„span
                        if adjusted_spans:
                            top_span = max(
                                adjusted_spans, key=lambda x: x[2]
                            )  # æŒ‰è°ƒæ•´åçš„durationæ’åº
                            (
                                span_id,
                                span_index,
                                adjusted_duration,
                                original_duration,
                            ) = top_span
                            span_duration_mapping[span_id] = (
                                original_duration  # å­˜å‚¨åŸå§‹æ—¶é—´ï¼Œåç»­ä¼šå†æ¬¡è°ƒæ•´
                            )

                            # é€šè¿‡span_indexä»span_listè·å–serviceNameå’ŒspanName
                            if span_list and 0 <= span_index < len(span_list):
                                span_info = span_list[span_index]
                                service_name, span_name = (
                                    self._extract_service_and_span_name(span_info)
                                )
                                if service_name and span_name:
                                    span_service_mapping[span_id] = (
                                        service_name,
                                        span_name,
                                    )
                    else:
                        # ä¸éœ€è¦å‡å»å¹³å‡å€¼çš„æƒ…å†µï¼šç›´æ¥æ‰¾åŸå§‹ç‹¬å æ—¶é—´æœ€é•¿çš„span
                        valid_spans = []
                        for span_id, span_index, duration in zip(
                            span_ids, span_indices, exclusive_durations
                        ):
                            if isinstance(duration, (int, float)) and duration > 0:
                                # æˆªæ–­å¼‚å¸¸é•¿çš„durationï¼Œå¤„ç†outliers
                                truncated_duration = min(duration, MAX_DURATION)
                                valid_spans.append(
                                    (span_id, span_index, truncated_duration)
                                )

                        # æ‰¾åˆ°åŸå§‹ç‹¬å æ—¶é—´æœ€é•¿çš„span
                        if valid_spans:
                            top_span = max(
                                valid_spans, key=lambda x: x[2]
                            )  # æŒ‰åŸå§‹durationæ’åº
                            span_id, span_index, duration = top_span
                            span_duration_mapping[span_id] = duration

                            # é€šè¿‡span_indexä»span_listè·å–serviceNameå’ŒspanName
                            if span_list and 0 <= span_index < len(span_list):
                                span_info = span_list[span_index]
                                service_name, span_name = (
                                    self._extract_service_and_span_name(span_info)
                                )
                                if service_name and span_name:
                                    span_service_mapping[span_id] = (
                                        service_name,
                                        span_name,
                                    )
                else:
                    # å¤„ç†æ‰€æœ‰spanï¼ˆåŸå§‹é€»è¾‘ï¼‰
                    for span_id, span_index, duration in zip(
                        span_ids, span_indices, exclusive_durations
                    ):
                        if isinstance(duration, (int, float)) and duration > 0:
                            # æˆªæ–­å¼‚å¸¸é•¿çš„durationï¼Œå¤„ç†outliers
                            truncated_duration = min(duration, MAX_DURATION)
                            span_duration_mapping[span_id] = truncated_duration

                            # é€šè¿‡span_indexä»span_listè·å–serviceNameå’ŒspanName
                            if span_list and 0 <= span_index < len(span_list):
                                span_info = span_list[span_index]
                                service_name, span_name = (
                                    self._extract_service_and_span_name(span_info)
                                )
                                if service_name and span_name:
                                    span_service_mapping[span_id] = (
                                        service_name,
                                        span_name,
                                    )

            except Exception as e:
                logger.error("å¤„ç†æ—¥å¿—æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: %s", e)
                continue

        if not span_duration_mapping:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç‹¬å æ—¶é—´æ•°æ®")
            return []

        logger.info("æ€»å…±æ‰¾åˆ° %d ä¸ªæœ‰æ•ˆçš„spanç‹¬å æ—¶é—´æ•°æ®", len(span_duration_mapping))
        logger.info(
            "æˆåŠŸæ˜ å°„ %d ä¸ªspançš„serviceNameå’ŒspanName", len(span_service_mapping)
        )

        # æ™ºèƒ½æ–¹æ¡ˆé€‰æ‹©ï¼šæ£€æŸ¥æ–¹æ¡ˆ1çš„æˆåŠŸç‡
        if self.minus_average and self.span_average_durations:
            if span_service_mapping:
                # è®¡ç®—æ–¹æ¡ˆ1çš„è¦†ç›–ç‡
                coverage_rate = len(span_service_mapping) / len(span_duration_mapping)
                logger.info(
                    "æ–¹æ¡ˆ1è¦†ç›–ç‡: %.2f%% (%d/%d)",
                    coverage_rate * 100,
                    len(span_service_mapping),
                    len(span_duration_mapping),
                )

                # å¦‚æœè¦†ç›–ç‡å¤§äº50%ï¼Œä½¿ç”¨æ–¹æ¡ˆ1ï¼›å¦åˆ™fallbackåˆ°æ–¹æ¡ˆ2
                if coverage_rate > 0.5:
                    logger.info(
                        "âœ… é€‰æ‹©æ–¹æ¡ˆ1ï¼šç›´æ¥ä½¿ç”¨span_listä¸­çš„serviceNameå’ŒspanNameï¼ˆæ¨èï¼‰"
                    )
                    adjusted_span_durations = self._adjust_durations_directly(
                        span_duration_mapping, span_service_mapping
                    )
                else:
                    logger.warning(
                        "âš ï¸  æ–¹æ¡ˆ1è¦†ç›–ç‡è¾ƒä½ï¼Œfallbackåˆ°æ–¹æ¡ˆ2ï¼šé‡æ–°æŸ¥è¯¢å¹¶é‡‡æ ·"
                    )
                    adjusted_span_durations = self._adjust_durations_with_span_average(
                        span_duration_mapping
                    )
            else:
                logger.warning(
                    "âš ï¸  æ–¹æ¡ˆ1å¤±è´¥ï¼šspan_listä¸­æ²¡æœ‰æ‰¾åˆ°serviceNameå’ŒspanNameï¼Œfallbackåˆ°æ–¹æ¡ˆ2"
                )
                adjusted_span_durations = self._adjust_durations_with_span_average(
                    span_duration_mapping
                )
        else:
            # å³ä½¿ä¸å‡å»å¹³å‡å€¼ï¼Œä¹Ÿè¦æˆªæ–­å¼‚å¸¸é•¿çš„durationï¼Œå¤„ç†outliers
            adjusted_span_durations = [
                (span_id, min(duration, MAX_DURATION))
                for span_id, duration in span_duration_mapping.items()
            ]

        # æŒ‰ç‹¬å æ—¶é—´é™åºæ’åº
        adjusted_span_durations.sort(key=lambda x: x[1], reverse=True)

        # è®¡ç®—æ€»ç‹¬å æ—¶é—´
        total_duration = sum(duration for _, duration in adjusted_span_durations)
        logger.info("æ€»ç‹¬å æ—¶é—´: %d", total_duration)

        if total_duration == 0:
            logger.warning("æ€»ç‹¬å æ—¶é—´ä¸º0ï¼Œæ— æ³•è®¡ç®—95%")
            return []

        # æ‰¾å‡ºå å‰95%çš„span
        cumulative_duration = 0
        target_duration = total_duration * PERCENT_95  # * 0.95
        top_95_percent_spans = []

        for span_id, duration in adjusted_span_durations:
            cumulative_duration += duration
            top_95_percent_spans.append(span_id)

            if cumulative_duration >= target_duration:
                break

        logger.info("å å‰95%%ç‹¬å æ—¶é—´çš„spanæ•°é‡: %d", len(top_95_percent_spans))
        logger.info(
            "è¿™äº›spançš„ç´¯è®¡ç‹¬å æ—¶é—´: %d, å æ€»æ—¶é—´çš„: %.2f%%",
            cumulative_duration,
            cumulative_duration / total_duration * 100,
        )

        return top_95_percent_spans

    def _adjust_durations_with_span_average(self, span_duration_mapping: dict) -> list:
        """
        æ ¹æ®serviceNameå’ŒspanNameç»„åˆçš„å¹³å‡å€¼è°ƒæ•´ç‹¬å æ—¶é—´
        ä¼˜åŒ–æ–¹æ¡ˆ2ï¼šé‡‡æ ·HIGH_RT_TRACESè¿™ä¹ˆå¤šä¸ªspanè¿›è¡Œè®¡ç®—

        Args:
            span_duration_mapping: {span_id: exclusive_duration}

        Returns:
            è°ƒæ•´åçš„(span_id, adjusted_duration)åˆ—è¡¨
        """
        logger.info("ğŸ”„ [æ–¹æ¡ˆ2] ä½¿ç”¨é‡‡æ ·æŸ¥è¯¢æ–¹æ¡ˆè¿›è¡Œè°ƒæ•´...")
        logger.info("ğŸ”„ [æ–¹æ¡ˆ2] é‡‡æ ·æœ€å¤š %d ä¸ªspanè¿›è¡ŒæŸ¥è¯¢", HIGH_RT_TRACES)

        adjusted_durations = []
        span_ids = list(span_duration_mapping.keys())

        # é‡‡æ ·ä¼˜åŒ–ï¼šå¦‚æœspanæ•°é‡è¶…è¿‡HIGH_RT_TRACESï¼Œéšæœºé‡‡æ ·
        if len(span_ids) > HIGH_RT_TRACES:
            sorted_span_ids = sorted(
                span_ids, key=lambda x: span_duration_mapping[x], reverse=True
            )
            sampled_span_ids = sorted_span_ids[:HIGH_RT_TRACES]
            logger.info(
                "ä» %d ä¸ªspanä¸­é‡‡æ ·äº† %d ä¸ªè¿›è¡ŒæŸ¥è¯¢",
                len(span_ids),
                len(sampled_span_ids),
            )
        else:
            sampled_span_ids = span_ids
            logger.info("spanæ•°é‡(%d)ä¸è¶…è¿‡é™åˆ¶ï¼ŒæŸ¥è¯¢æ‰€æœ‰span", len(span_ids))

        # åˆ†æ‰¹æŸ¥è¯¢ï¼Œå¢å¤§æ‰¹æ¬¡å¤§å°ä»¥æé«˜æ€§èƒ½
        batch_size = 500  # å¢å¤§æ‰¹æ¬¡å¤§å°
        span_service_mapping = {}  # å­˜å‚¨span_idåˆ°(serviceName, spanName)çš„æ˜ å°„

        for i in range(0, len(sampled_span_ids), batch_size):
            batch_span_ids = sampled_span_ids[i : i + batch_size]
            span_conditions = " or ".join(
                [f"spanId='{span_id}'" for span_id in batch_span_ids]
            )
            service_query = f"* | select spanId, serviceName, spanName from log where {span_conditions}"

            request = GetLogsRequest(
                project=self.project_name,
                logstore=self.logstore_name,
                query=service_query,
                fromTime=self.start_time,
                toTime=self.end_time,
                # line=HIGH_RT_TRACES  # ä½¿ç”¨HIGH_RT_TRACESä½œä¸ºæŸ¥è¯¢é™åˆ¶
            )

            try:
                logger.info(
                    "æŸ¥è¯¢ç¬¬ %d æ‰¹ï¼Œå…± %d ä¸ªspançš„serviceNameå’ŒspanName...",
                    i // batch_size + 1,
                    len(batch_span_ids),
                )
                response = self.client.get_logs(request)
                service_logs = (
                    [log_item.get_contents() for log_item in response.get_logs()]
                    if response
                    else []
                )
                logger.info("æŸ¥è¯¢åˆ° %d æ¡è®°å½•", len(service_logs))

                # æ„å»ºspan_idåˆ°(serviceName, spanName)çš„æ˜ å°„
                for log in service_logs:
                    span_id = log.get("spanId")
                    service_name = log.get("serviceName")
                    span_name = log.get("spanName")
                    if span_id and service_name and span_name:
                        span_service_mapping[span_id] = (service_name, span_name)

            except Exception as e:
                logger.error("æŸ¥è¯¢ç¬¬ %d æ‰¹æ—¶å‘ç”Ÿé”™è¯¯: %s", i // batch_size + 1, e)
                continue

        logger.info(
            "æˆåŠŸæ˜ å°„ %d ä¸ªspançš„serviceNameå’ŒspanName", len(span_service_mapping)
        )

        # ä½¿ç”¨forå¾ªç¯åœ¨æœ¬åœ°è®¡ç®—è°ƒæ•´åçš„æ—¶é—´
        logger.info("å¼€å§‹æœ¬åœ°è®¡ç®—è°ƒæ•´åçš„ç‹¬å æ—¶é—´...")
        for span_id, original_duration in span_duration_mapping.items():
            # å¦‚æœè¿™ä¸ªspan_idåœ¨é‡‡æ ·èŒƒå›´å†…ä¸”æœ‰æ˜ å°„ä¿¡æ¯ï¼Œåˆ™ä½¿ç”¨å¹³å‡å€¼è°ƒæ•´
            if span_id in span_service_mapping:
                service_name, span_name = span_service_mapping[span_id]
                # æ„å»ºç»„åˆé”®
                combined_key = f"{service_name}<sep>{span_name}"

                if combined_key in self.span_average_durations:
                    avg_duration = self.span_average_durations[combined_key]
                    adjusted_duration = max(
                        0, original_duration - avg_duration
                    )  # ç¡®ä¿ä¸ä¸ºè´Ÿæ•°

                    adjusted_duration = min(adjusted_duration, MAX_DURATION)
                    # print(f"span {span_id} æœåŠ¡ {service_name} spanName {span_name}: åŸå§‹æ—¶é—´={original_duration}, å¹³å‡æ—¶é—´={avg_duration:.2f}, è°ƒæ•´åæ—¶é—´={adjusted_duration:.2f}")
                else:
                    adjusted_duration = original_duration
                    # print(f"span {span_id} æœåŠ¡ {service_name} spanName {span_name}: æ²¡æœ‰å¹³å‡å€¼ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹æ—¶é—´={original_duration}")
            else:
                # æ²¡æœ‰åœ¨é‡‡æ ·èŒƒå›´å†…æˆ–æ²¡æœ‰æ‰¾åˆ°serviceNameå’ŒspanNameï¼Œä½¿ç”¨åŸå§‹æ—¶é—´
                adjusted_duration = original_duration
                # print(f"span {span_id}: æ²¡æœ‰åœ¨é‡‡æ ·èŒƒå›´å†…æˆ–æ²¡æœ‰æ‰¾åˆ°serviceNameå’ŒspanNameï¼Œä½¿ç”¨åŸå§‹æ—¶é—´={original_duration}")

            adjusted_durations.append((span_id, adjusted_duration))

        logger.info("å®Œæˆ %d ä¸ªspançš„æ—¶é—´è°ƒæ•´è®¡ç®—", len(adjusted_durations))
        return adjusted_durations

    def _adjust_durations_directly(
        self, span_duration_mapping: dict, span_service_mapping: dict
    ) -> list:
        """
        æ ¹æ®serviceNameå’ŒspanNameç»„åˆçš„å¹³å‡å€¼è°ƒæ•´ç‹¬å æ—¶é—´
        æ–¹æ¡ˆ1ï¼šç›´æ¥ä½¿ç”¨span_listä¸­çš„serviceNameå’ŒspanNameï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰

        Args:
            span_duration_mapping: {span_id: exclusive_duration}
            span_service_mapping: {span_id: (serviceName, spanName)}

        Returns:
            è°ƒæ•´åçš„(span_id, adjusted_duration)åˆ—è¡¨
        """
        logger.info("ğŸš€ [æ–¹æ¡ˆ1] ä½¿ç”¨span_listä¸­çš„serviceNameå’ŒspanNameè¿›è¡Œè°ƒæ•´...")
        logger.info(
            "ğŸš€ [æ–¹æ¡ˆ1] æ— éœ€é¢å¤–æŸ¥è¯¢ï¼Œç›´æ¥å¤„ç† %d ä¸ªspan", len(span_duration_mapping)
        )

        adjusted_durations = []
        span_ids = list(span_duration_mapping.keys())

        # ä½¿ç”¨forå¾ªç¯åœ¨æœ¬åœ°è®¡ç®—è°ƒæ•´åçš„æ—¶é—´
        logger.info("å¼€å§‹æœ¬åœ°è®¡ç®—è°ƒæ•´åçš„ç‹¬å æ—¶é—´...")
        for span_id, original_duration in span_duration_mapping.items():
            span_info = span_service_mapping.get(span_id)

            if span_info:
                service_name, span_name = span_info
                # æ„å»ºç»„åˆé”®
                combined_key = f"{service_name}<sep>{span_name}"

                if combined_key in self.span_average_durations:
                    avg_duration = self.span_average_durations[combined_key]
                    adjusted_duration = max(
                        0, original_duration - avg_duration
                    )  # ç¡®ä¿ä¸ä¸ºè´Ÿæ•°

                    adjusted_duration = min(adjusted_duration, MAX_DURATION)
                    # print(f"span {span_id} æœåŠ¡ {service_name} spanName {span_name}: åŸå§‹æ—¶é—´={original_duration}, å¹³å‡æ—¶é—´={avg_duration:.2f}, è°ƒæ•´åæ—¶é—´={adjusted_duration:.2f}")
                else:
                    adjusted_duration = original_duration
                    # print(f"span {span_id} æœåŠ¡ {service_name} spanName {span_name}: æ²¡æœ‰å¹³å‡å€¼ä¿¡æ¯ï¼Œä½¿ç”¨åŸå§‹æ—¶é—´={original_duration}")
            else:
                # æ²¡æœ‰æ‰¾åˆ°serviceNameå’ŒspanNameï¼Œä½¿ç”¨åŸå§‹æ—¶é—´
                adjusted_duration = original_duration
                # print(f"span {span_id}: æ²¡æœ‰æ‰¾åˆ°serviceNameå’ŒspanNameï¼Œä½¿ç”¨åŸå§‹æ—¶é—´={original_duration}")

            adjusted_durations.append((span_id, adjusted_duration))

        logger.info("å®Œæˆ %d ä¸ªspançš„æ—¶é—´è°ƒæ•´è®¡ç®—", len(adjusted_durations))
        return adjusted_durations

    def _extract_service_and_span_name(self, span_info):
        """
        ä»span_infoä¸­æå–serviceNameå’ŒspanName
        å¤„ç†å¤šç§å¯èƒ½çš„æ•°æ®æ ¼å¼

        Args:
            span_info: spanä¿¡æ¯ï¼Œå¯èƒ½æ˜¯dictã€listã€tupleç­‰æ ¼å¼

        Returns:
            tuple: (service_name, span_name)
        """
        try:
            # æ ¼å¼1ï¼šå­—å…¸æ ¼å¼
            if isinstance(span_info, dict):
                service_name = span_info.get("servicename", "") or span_info.get(
                    "serviceName", ""
                )
                span_name = span_info.get("spanname", "") or span_info.get(
                    "spanName", ""
                )
                return service_name, span_name

            # æ ¼å¼2ï¼šåˆ—è¡¨/å…ƒç»„æ ¼å¼ï¼ŒæŒ‰ç…§SPLæŸ¥è¯¢ä¸­çš„å­—æ®µé¡ºåº
            # é¡ºåºï¼šspanid, parentspanid, statuscode, servicename, spanname, hostname, kind, ...
            elif isinstance(span_info, (list, tuple)) and len(span_info) >= 5:
                service_name = span_info[3] if len(span_info) > 3 else ""
                span_name = span_info[4] if len(span_info) > 4 else ""
                return service_name, span_name

            # æ ¼å¼3ï¼šå­—ç¬¦ä¸²æ ¼å¼ï¼ˆå¯èƒ½æ˜¯JSONå­—ç¬¦ä¸²ï¼‰
            elif isinstance(span_info, str):
                import json

                try:
                    parsed_info = json.loads(span_info)
                    return self._extract_service_and_span_name(parsed_info)  # é€’å½’å¤„ç†
                except json.JSONDecodeError:
                    pass

            logger.warning("æ— æ³•è§£æspan_infoæ ¼å¼: %s", type(span_info))
            return "", ""

        except Exception as e:
            logger.error("æå–serviceNameå’ŒspanNameæ—¶å‘ç”Ÿé”™è¯¯: %s", e)
            return "", ""

    def _parse_array_field(self, field_value: str) -> list:
        """
        è§£ææ•°ç»„å­—æ®µï¼Œæ”¯æŒJSONæ ¼å¼çš„æ•°ç»„

        Args:
            field_value: å­—æ®µå€¼ï¼Œå¯èƒ½æ˜¯JSONæ•°ç»„æ ¼å¼çš„å­—ç¬¦ä¸²

        Returns:
            è§£æåçš„åˆ—è¡¨
        """
        import json

        if not field_value or field_value == "[]":
            return []

        try:
            # å°è¯•ç›´æ¥è§£æJSON
            return json.loads(field_value)
        except json.JSONDecodeError:
            try:
                # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•å¤„ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜
                cleaned_value = field_value.strip()
                if cleaned_value.startswith("[") and cleaned_value.endswith("]"):
                    return json.loads(cleaned_value)
                else:
                    # å¦‚æœä¸æ˜¯æ•°ç»„æ ¼å¼ï¼Œå°è¯•æŒ‰é€—å·åˆ†å‰²
                    return [
                        item.strip().strip("\"'")
                        for item in cleaned_value.split(",")
                        if item.strip()
                    ]
            except:
                logger.warning("æ— æ³•è§£ææ•°ç»„å­—æ®µ: %s", field_value)
                return []

    def get_top_95_percent_spans_query(self) -> tuple[str, str]:
        """
        è·å–æŸ¥è¯¢å‰95%ç‹¬å æ—¶é—´spanè¯¦ç»†ä¿¡æ¯çš„SPLæŸ¥è¯¢è¯­å¥

        Returns:
            (span_conditions, query): æŸ¥è¯¢æ¡ä»¶å’Œå®Œæ•´çš„æŸ¥è¯¢è¯­å¥
        """
        top_spans = self.find_top_95_percent_spans()

        if not top_spans:
            return "", "* | select * from log where false"  # è¿”å›ç©ºç»“æœçš„æŸ¥è¯¢

        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        span_conditions = " or ".join([f"spanId='{span_id}'" for span_id in top_spans])
        query = f"* | select * from log where {span_conditions}"

        return span_conditions, query

    def _calculate_span_averages(self):
        """
        è®¡ç®—æ­£å¸¸æ—¶é—´æ®µå†…æ¯ä¸ªserviceName<sep>spanNameç»„åˆçš„å¹³å‡ç‹¬å æ—¶é—´
        """

        # é¦–å…ˆè·å–ç‹¬å æ—¶é—´æ•°æ®
        logger.info("è·å–ç‹¬å æ—¶é—´æ•°æ®...")
        # exclusive_duration_query = get_spl_query(self.duration_threshold, TRACES_FOR_AVG_RT)
        exclusive_duration_query = get_spl_query(0, TRACES_FOR_AVG_RT)

        request = GetLogsRequest(
            project=self.project_name,
            logstore=self.logstore_name,
            query=exclusive_duration_query,
            fromTime=self.normal_start_time,
            toTime=self.normal_end_time,
            # line=TRACES_FOR_AVG_RT # only calculate for 1000 logs
        )

        try:
            response = self.client.get_logs(request)
            logs = (
                [log_item.get_contents() for log_item in response.get_logs()]
                if response
                else []
            )
            logger.info("æ­£å¸¸æ—¶é—´æ®µæŸ¥è¯¢åˆ°çš„ç‹¬å æ—¶é—´æ—¥å¿—æ¡æ•°: %d", len(logs))

            # æ”¶é›†æ‰€æœ‰çš„span_idå’Œå¯¹åº”çš„ç‹¬å æ—¶é—´
            span_duration_mapping = {}  # {span_id: exclusive_duration}

            logger.info("å¼€å§‹è®¡ç®—æ­£å¸¸æ—¶é—´æ®µçš„å¹³å‡ç‹¬å æ—¶é—´...")

            for log in logs:
                try:
                    span_ids = self._parse_array_field(log.get("span_id", "[]"))
                    exclusive_durations = self._parse_array_field(
                        log.get("exclusive_duration", "[]")
                    )

                    if len(span_ids) != len(exclusive_durations):
                        continue

                    for span_id, duration in zip(span_ids, exclusive_durations):
                        if isinstance(duration, (int, float)) and duration > 0:
                            span_duration_mapping[span_id] = duration

                except Exception as e:
                    logger.error("å¤„ç†ç‹¬å æ—¶é—´æ—¥å¿—æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: %s", e)
                    continue

            logger.info("æ”¶é›†åˆ° %d ä¸ªspançš„ç‹¬å æ—¶é—´ä¿¡æ¯", len(span_duration_mapping))

            # ç„¶åæŸ¥è¯¢è¿™äº›spançš„spanNameä¿¡æ¯
            if span_duration_mapping:
                self._query_span_names_for_spans(span_duration_mapping)

        except Exception as e:
            logger.error("è®¡ç®—å¹³å‡å€¼æ—¶å‘ç”Ÿé”™è¯¯: %s", e)

    def _query_span_names_for_spans(self, span_duration_mapping: dict):
        """
        æŸ¥è¯¢æŒ‡å®šspan_idçš„serviceNameå’ŒspanNameä¿¡æ¯
        ä¼˜åŒ–ï¼šé‡‡æ ·TRACES_FOR_AVG_RTè¿™ä¹ˆå¤šä¸ªspanæ¥è®¡ç®—å¹³å‡å€¼
        """
        logger.info("æŸ¥è¯¢spançš„serviceNameå’ŒspanNameä¿¡æ¯...")

        # é‡‡æ ·ä¼˜åŒ–ï¼šéšæœºé‡‡æ · TRACES_FOR_AVG_RT è¿™ä¹ˆå¤šä¸ªspanæ¥è®¡ç®—å¹³å‡å€¼
        span_ids = list(span_duration_mapping.keys())
        if len(span_ids) > TRACES_FOR_AVG_RT:
            # span_ids = random.sample(span_ids, TRACES_FOR_AVG_RT)
            sorted_span_ids = sorted(
                span_ids, key=lambda x: span_duration_mapping[x], reverse=True
            )
            span_ids = sorted_span_ids[:TRACES_FOR_AVG_RT]
            span_duration_mapping = {
                span_id: span_duration_mapping[span_id] for span_id in span_ids
            }
            logger.info("ä»åŸå§‹spanä¸­é‡‡æ ·äº† %d ä¸ªç”¨äºè®¡ç®—å¹³å‡å€¼", len(span_ids))
        else:
            logger.info("spanæ•°é‡(%d)ä¸è¶…è¿‡é™åˆ¶ï¼Œä½¿ç”¨æ‰€æœ‰spanè®¡ç®—å¹³å‡å€¼", len(span_ids))

        # æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼Œåˆ†æ‰¹æŸ¥è¯¢ä½†å¢å¤§æ‰¹æ¬¡å¤§å°
        batch_size = 500  # å¢å¤§æ‰¹æ¬¡å¤§å°ä»¥æé«˜æ€§èƒ½ï¼Œä½†é¿å…æŸ¥è¯¢æ¡ä»¶è¿‡é•¿
        service_durations = {}  # {serviceName<sep>spanName: [duration1, duration2, ...]}

        for i in range(0, len(span_ids), batch_size):
            batch_span_ids = span_ids[i : i + batch_size]
            span_conditions = " or ".join(
                [f"spanId='{span_id}'" for span_id in batch_span_ids]
            )

            service_query = f"* | select spanId, serviceName, spanName from log  where {span_conditions}"

            request = GetLogsRequest(
                project=self.project_name,
                logstore=self.logstore_name,
                query=service_query,
                fromTime=self.normal_start_time,
                toTime=self.normal_end_time,
                # line=TRACES_FOR_AVG_RT  # ä½¿ç”¨TRACES_FOR_AVG_RTä½œä¸ºæŸ¥è¯¢é™åˆ¶
            )

            try:
                logger.info(
                    "æŸ¥è¯¢ç¬¬ %d æ‰¹ï¼Œå…± %d ä¸ªspan...",
                    i // batch_size + 1,
                    len(batch_span_ids),
                )
                response = self.client.get_logs(request)
                service_logs = (
                    [log_item.get_contents() for log_item in response.get_logs()]
                    if response
                    else []
                )
                logger.info("æŸ¥è¯¢åˆ° %d æ¡è®°å½•", len(service_logs))

                # ä½¿ç”¨forå¾ªç¯åœ¨æœ¬åœ°è®¡ç®—
                for log in service_logs:
                    span_id = log.get("spanId")
                    service_name = log.get("serviceName")
                    span_name = log.get("spanName")

                    if span_id in span_duration_mapping and service_name and span_name:
                        duration = span_duration_mapping[span_id]
                        # æ„å»ºç»„åˆé”®
                        combined_key = f"{service_name}<sep>{span_name}"
                        if combined_key not in service_durations:
                            service_durations[combined_key] = []
                        service_durations[combined_key].append(duration)

            except Exception as e:
                logger.error("æŸ¥è¯¢ç¬¬ %d æ‰¹æ—¶å‘ç”Ÿé”™è¯¯: %s", i // batch_size + 1, e)
                continue

        # è®¡ç®—æ¯ä¸ªserviceName<sep>spanNameç»„åˆçš„å¹³å‡å€¼
        for combined_key, durations in service_durations.items():
            if durations:
                avg_duration = np.mean(durations)
                self.span_average_durations[combined_key] = avg_duration
                logger.info(
                    "ç»„åˆé”® %s çš„å¹³å‡ç‹¬å æ—¶é—´: %.2f", combined_key, avg_duration
                )

        logger.info(
            "å…±è®¡ç®—äº† %d ä¸ªserviceName<sep>spanNameç»„åˆçš„å¹³å‡ç‹¬å æ—¶é—´",
            len(self.span_average_durations),
        )


# def test_optimized_versions():
#     """æµ‹è¯•è‡ªåŠ¨æ–¹æ¡ˆé€‰æ‹©æœºåˆ¶"""
#     print("="*60)
#     print("æµ‹è¯•è‡ªåŠ¨æ–¹æ¡ˆé€‰æ‹©æœºåˆ¶:")
#     print("="*60)

#     print("ç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆï¼š")
#     print("1. é¦–å…ˆå°è¯•æ–¹æ¡ˆ1ï¼šç›´æ¥ä»span_listè·å–serviceNameå’ŒspanName")
#     print("2. è®¡ç®—æ–¹æ¡ˆ1çš„è¦†ç›–ç‡ï¼ˆæˆåŠŸæ˜ å°„çš„spanæ¯”ä¾‹ï¼‰")
#     print("3. å¦‚æœè¦†ç›–ç‡ > 50%ï¼Œä½¿ç”¨æ–¹æ¡ˆ1ï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰")
#     print("4. å¦‚æœè¦†ç›–ç‡ â‰¤ 50%ï¼Œè‡ªåŠ¨fallbackåˆ°æ–¹æ¡ˆ2ï¼ˆé‡‡æ ·æŸ¥è¯¢ï¼‰")
#     print("5. å¦‚æœæ–¹æ¡ˆ1å®Œå…¨å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨æ–¹æ¡ˆ2")
#     print("\n" + "-" * 50)

#     finder = FindRootCauseSpansRT(
#         project_name="proj-xtrace-ee483ec157740929c4cb92d4ff85f-cn-qingdao",
#         logstore_name="logstore-tracing",
#         region="cn-qingdao",
#         start_time="2025-06-29 18:36:01",
#         end_time="2025-06-29 18:41:01",
#         duration_threshold=1000000,  # 1ç§’
#         limit_num=1000,
#         normal_start_time="2025-06-29 18:25:31",
#         normal_end_time="2025-06-29 18:35:01",
#         minus_average=True  # å¯ç”¨å‡å»å¹³å‡å€¼çš„åŠŸèƒ½
#     )

#     print("\nå¼€å§‹æŸ¥æ‰¾å‰95%ç‹¬å æ—¶é—´çš„span...")
#     print("ç³»ç»Ÿå°†è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ:")
#     top_spans = finder.find_top_95_percent_spans()
#     print(f"\nç»“æœï¼šæ‰¾åˆ° {len(top_spans)} ä¸ªå å‰95%ç‹¬å æ—¶é—´çš„span")

#     print("\n" + "="*60)
#     print("æ–¹æ¡ˆé€‰æ‹©æœºåˆ¶è¯´æ˜:")
#     print("âœ… æ–¹æ¡ˆ1æˆåŠŸ â†’ ç›´æ¥ä½¿ç”¨span_listï¼Œæ— é¢å¤–æŸ¥è¯¢ï¼Œæ€§èƒ½æœ€ä¼˜")
#     print("âš ï¸  æ–¹æ¡ˆ1éƒ¨åˆ†æˆåŠŸ â†’ æ ¹æ®è¦†ç›–ç‡å†³å®šæ˜¯å¦fallback")
#     print("âŒ æ–¹æ¡ˆ1å¤±è´¥ â†’ è‡ªåŠ¨ä½¿ç”¨æ–¹æ¡ˆ2ï¼Œé‡‡æ ·æŸ¥è¯¢ï¼Œä¿è¯åŠŸèƒ½")
#     print("="*60)


# def test_find_root_cause_spans_rt():
#     """æµ‹è¯•å‡½æ•°"""
#     finder = FindRootCauseSpansRT(
#         project_name="proj-xtrace-ee483ec157740929c4cb92d4ff85f-cn-qingdao",
#         logstore_name="logstore-tracing",
#         region="cn-qingdao",
#         start_time="2025-06-29 18:36:01",
#         end_time="2025-06-29 18:41:01",
#         duration_threshold=1000000,  # 1ç§’
#         limit_num=1000,
#         normal_start_time="2025-06-29 18:25:31",
#         normal_end_time="2025-06-29 18:35:01",
#         minus_average=True  # å¯ç”¨å‡å»å¹³å‡å€¼çš„åŠŸèƒ½
#     )

#     # è·å–å‰95%çš„span_id
#     top_spans = finder.find_top_95_percent_spans()
#     print(f"å‰95%ç‹¬å æ—¶é—´çš„span_id: {top_spans}")

#     # è·å–æŸ¥è¯¢è¿™äº›spanè¯¦ç»†ä¿¡æ¯çš„æŸ¥è¯¢è¯­å¥
#     span_conditions, query = finder.get_top_95_percent_spans_query()
#     print(f"æŸ¥è¯¢æ¡ä»¶: {span_conditions}")

#     # æ‰“å°æœåŠ¡å¹³å‡å€¼ä¿¡æ¯
#     print("\nå„serviceName<sep>spanNameç»„åˆçš„å¹³å‡ç‹¬å æ—¶é—´:")
#     for combined_key, avg_duration in finder.span_average_durations.items():
#         print(f"  {combined_key}: {avg_duration:.2f}")


# def test_without_minus_average():
#     """æµ‹è¯•ä¸å‡å»å¹³å‡å€¼çš„æƒ…å†µ"""
#     print("="*50)
#     print("æµ‹è¯•ä¸å‡å»å¹³å‡å€¼çš„æƒ…å†µ:")
#     print("="*50)

#     finder = FindRootCauseSpansRT(
#         project_name="proj-xtrace-ee483ec157740929c4cb92d4ff85f-cn-qingdao",
#         logstore_name="logstore-tracing",
#         region="cn-qingdao",
#         start_time="2025-06-29 18:36:01",
#         end_time="2025-06-29 18:41:01",
#         duration_threshold=1000000,  # 1ç§’
#         limit_num=1000,
#         minus_average=False  # ä¸å‡å»å¹³å‡å€¼
#     )

#     top_spans = finder.find_top_95_percent_spans()
#     print(f"å‰95%ç‹¬å æ—¶é—´çš„span_id (ä¸å‡å¹³å‡å€¼): {top_spans}")


# def test_only_top1_per_trace():
#     """æµ‹è¯•åªå¤„ç†æ¯ä¸ªtraceä¸­ç‹¬å æ—¶é—´æ’top-1çš„span"""
#     print("="*60)
#     print("æµ‹è¯•åªå¤„ç†æ¯ä¸ªtraceä¸­ç‹¬å æ—¶é—´æ’top-1çš„span:")
#     print("="*60)

#     print("å¼€å…³è¯´æ˜:")
#     print("â€¢ only_top1_per_trace=False: å¤„ç†æ¯ä¸ªtraceä¸­çš„æ‰€æœ‰spanï¼ˆé»˜è®¤è¡Œä¸ºï¼‰")
#     print("â€¢ only_top1_per_trace=True:  åªå¤„ç†æ¯ä¸ªtraceä¸­ç‹¬å æ—¶é—´æœ€é•¿çš„span")
#     print("â€¢ è¿™å¯ä»¥æ˜¾è‘—å‡å°‘è®¡ç®—é‡ï¼Œèšç„¦äºæ¯ä¸ªtraceçš„æœ€è€—æ—¶æ“ä½œ")
#     print()

#     # æµ‹è¯•å¼€å…³å…³é—­çš„æƒ…å†µ
#     print("-" * 40)
#     print("1. å¼€å…³å…³é—­ (å¤„ç†æ‰€æœ‰span):")
#     print("-" * 40)

#     finder_all = FindRootCauseSpansRT(
#         project_name="proj-xtrace-ee483ec157740929c4cb92d4ff85f-cn-qingdao",
#         logstore_name="logstore-tracing",
#         region="cn-qingdao",
#         start_time="2025-06-29 18:36:01",
#         end_time="2025-06-29 18:41:01",
#         duration_threshold=1000000,  # 1ç§’
#         limit_num=1000,
#         only_top1_per_trace=False  # å¤„ç†æ‰€æœ‰span
#     )

#     top_spans_all = finder_all.find_top_95_percent_spans()
#     print(f"å¤„ç†æ‰€æœ‰spanæ—¶æ‰¾åˆ°çš„å‰95%spanæ•°é‡: {len(top_spans_all)}")

#     # æµ‹è¯•å¼€å…³æ‰“å¼€çš„æƒ…å†µ
#     print()
#     print("-" * 40)
#     print("2. å¼€å…³æ‰“å¼€ (æ¯ä¸ªtraceåªå–top-1):")
#     print("-" * 40)

#     finder_top1 = FindRootCauseSpansRT(
#         project_name="proj-xtrace-ee483ec157740929c4cb92d4ff85f-cn-qingdao",
#         logstore_name="logstore-tracing",
#         region="cn-qingdao",
#         start_time="2025-06-29 18:36:01",
#         end_time="2025-06-29 18:41:01",
#         duration_threshold=1000000,  # 1ç§’
#         limit_num=1000,
#         only_top1_per_trace=True   # åªå¤„ç†æ¯ä¸ªtraceä¸­top-1çš„span
#     )

#     top_spans_top1 = finder_top1.find_top_95_percent_spans()
#     print(f"åªå¤„ç†top-1 spanæ—¶æ‰¾åˆ°çš„å‰95%spanæ•°é‡: {len(top_spans_top1)}")

#     print()
#     print("=" * 60)
#     print("å¯¹æ¯”ç»“æœ:")
#     print(f"â€¢ å¤„ç†æ‰€æœ‰span: {len(top_spans_all)} ä¸ªspan")
#     print(f"â€¢ åªå¤„ç†top-1: {len(top_spans_top1)} ä¸ªspan")
#     if len(top_spans_all) > 0:
#         reduction_rate = (len(top_spans_all) - len(top_spans_top1)) / len(top_spans_all) * 100
#         print(f"â€¢ è®¡ç®—é‡å‡å°‘: {reduction_rate:.1f}%")
#     print("=" * 60)


# def test_top1_with_minus_average_logic():
#     """æµ‹è¯•only_top1_per_trace=Trueä¸”minus_average=Trueæ—¶çš„é€»è¾‘æ­£ç¡®æ€§"""
#     print("="*70)
#     print("æµ‹è¯• only_top1_per_trace + minus_average çš„é€»è¾‘æ­£ç¡®æ€§:")
#     print("="*70)

#     print("é€»è¾‘ä¿®å¤è¯´æ˜:")
#     print("â€¢ ä¿®å¤å‰ï¼šå…ˆæ‰¾åŸå§‹æ—¶é—´æœ€é•¿çš„spanï¼Œå†å‡å»å¹³å‡å€¼")
#     print("â€¢ ä¿®å¤åï¼šå…ˆå¯¹æ‰€æœ‰spanå‡å»å¹³å‡å€¼ï¼Œå†æ‰¾è°ƒæ•´åæ—¶é—´æœ€é•¿çš„span")
#     print("â€¢ è¿™ç¡®ä¿äº†é€‰æ‹©çš„æ˜¯ç›¸å¯¹äºæ­£å¸¸æƒ…å†µå¼‚å¸¸å¢åŠ æœ€å¤šçš„span")
#     print()

#     print("åœºæ™¯ä¸¾ä¾‹:")
#     print("å‡è®¾æŸä¸ªtraceæœ‰3ä¸ªspan:")
#     print("  spanA: åŸå§‹æ—¶é—´=10s, å¹³å‡å€¼=2s  â†’ è°ƒæ•´å=8s")
#     print("  spanB: åŸå§‹æ—¶é—´=12s, å¹³å‡å€¼=10s â†’ è°ƒæ•´å=2s")
#     print("  spanC: åŸå§‹æ—¶é—´=8s,  å¹³å‡å€¼=1s  â†’ è°ƒæ•´å=7s")
#     print()
#     print("ä¿®å¤å‰é€»è¾‘: ä¼šé€‰æ‹©spanB (åŸå§‹æ—¶é—´æœ€é•¿=12s)")
#     print("ä¿®å¤åé€»è¾‘: ä¼šé€‰æ‹©spanA (è°ƒæ•´åæ—¶é—´æœ€é•¿=8s) âœ…")
#     print("ä¿®å¤åçš„é€‰æ‹©æ›´åˆç†ï¼Œå› ä¸ºspanAç›¸å¯¹æ­£å¸¸æƒ…å†µå¼‚å¸¸å¢åŠ æœ€å¤š")
#     print()

#     # å®é™…æµ‹è¯•
#     print("-" * 50)
#     print("å®é™…æµ‹è¯•ï¼š")
#     print("-" * 50)

#     finder = FindRootCauseSpansRT(
#         project_name="proj-xtrace-ee483ec157740929c4cb92d4ff85f-cn-qingdao",
#         logstore_name="logstore-tracing",
#         region="cn-qingdao",
#         start_time="2025-06-29 18:36:01",
#         end_time="2025-06-29 18:41:01",
#         duration_threshold=1000000,  # 1ç§’
#         limit_num=1000,
#         normal_start_time="2025-06-29 18:25:31",
#         normal_end_time="2025-06-29 18:35:01",
#         minus_average=True,           # å¯ç”¨å‡å»å¹³å‡å€¼
#         only_top1_per_trace=True      # åªå¤„ç†æ¯ä¸ªtraceçš„top-1
#     )

#     print("å¼€å§‹æµ‹è¯•ä¿®å¤åçš„é€»è¾‘...")
#     top_spans = finder.find_top_95_percent_spans()
#     print(f"ç»“æœï¼šæ‰¾åˆ° {len(top_spans)} ä¸ªspan")
#     print("âœ… è¿™äº›spanæ˜¯åŸºäºè°ƒæ•´åç‹¬å æ—¶é—´ï¼ˆå‡å»å¹³å‡å€¼åï¼‰é€‰æ‹©çš„top-1")

#     print()
#     print("=" * 70)
#     print("é€»è¾‘ä¿®å¤å®Œæˆï¼ç°åœ¨ç³»ç»Ÿä¼šæ­£ç¡®é€‰æ‹©ç›¸å¯¹å¼‚å¸¸å¢åŠ æœ€å¤šçš„span")
#     print("=" * 70)


if __name__ == "__main__":
    pass
    # test_find_root_cause_spans_rt()
    # test_without_minus_average()
    # test_only_top1_per_trace()
    # test_top1_with_minus_average_logic()
    # test_optimized_versions()
