#!/usr/bin/env python
# coding: utf-8

"""
Latency Root Cause Analysis Module

This module implements comprehensive latency root cause analysis.
It provides a function to analyze latency issues and return root cause candidates.
"""

import os
import sys
import time
import json
from datetime import datetime, timedelta

# å°†çˆ¶ç›®å½•æ·»åŠ åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.append('..')

def analyze_services_for_anomalies(services_to_check, cpu_candidates, memory_candidates, candidate_root_causes,
                                   cms_tester, from_time, to_time, analysis_type="åˆ†æ"):
    """
    åˆ†æç»™å®šæœåŠ¡åˆ—è¡¨çš„CPUå’Œå†…å­˜å¼‚å¸¸

    Args:
        services_to_check: è¦æ£€æŸ¥çš„æœåŠ¡åˆ—è¡¨ [(service_name, count), ...]
        cpu_candidates: CPUå€™é€‰æœåŠ¡é›†åˆ
        memory_candidates: å†…å­˜å€™é€‰æœåŠ¡é›†åˆ
        candidate_root_causes: åŸå§‹å€™é€‰æ ¹å› åˆ—è¡¨
        cms_tester: CMSæµ‹è¯•å®¢æˆ·ç«¯
        from_time, to_time: æ—¶é—´èŒƒå›´
        analysis_type: åˆ†æç±»å‹æ ‡è¯†

    Returns:
        list: æ ¹å› å€™é€‰æ•°ç»„æˆ–ç©ºåˆ—è¡¨
    """
    if not services_to_check:
        return []

    print(f"\nğŸ“Š å¼€å§‹{analysis_type}æœåŠ¡å¼‚å¸¸æ£€æµ‹...")

    # æ­¥éª¤1ï¼šæ”¶é›†æ‰€æœ‰æœåŠ¡çš„CPUå¼‚å¸¸ä¿¡æ¯
    print(f"ğŸ” æ”¶é›†æ‰€æœ‰æœåŠ¡çš„CPUå¼‚å¸¸ä¿¡æ¯...")
    cpu_anomalies = []

    for service_name, pattern_count in services_to_check:
        print(f"  æ£€æŸ¥ {service_name} çš„CPUå¼‚å¸¸...")

        try:
            cpu_anomaly_detection_query = f"""
.entity_set with(domain='k8s', name='k8s.deployment', query=`name='{service_name}'` )
| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')
| extend ret = series_decompose_anomalies(__value__, '{{"confidence": 0.035}}')
| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg
"""

            cpu_anomaly_result = cms_tester._execute_spl_query(
                cpu_anomaly_detection_query.strip(),
                from_time=from_time,
                to_time=to_time
            )

            if cpu_anomaly_result and cpu_anomaly_result.data:
                cpu_anomaly_count = 0
                for record in cpu_anomaly_result.data:
                    if isinstance(record, (list, tuple)):
                        for item in record:
                            if isinstance(item, str):
                                exceed_upper_count = item.count('ExceedUpperBound')
                                cpu_anomaly_count += exceed_upper_count

                cpu_candidate = f"{service_name}.cpu"
                is_candidate_match = cpu_candidate in candidate_root_causes
                if not is_candidate_match:
                    is_candidate_match = service_name in cpu_candidates
                cpu_anomalies.append((service_name, cpu_anomaly_count, is_candidate_match, cpu_candidate))

                status = "âœ… åŒ¹é…" if is_candidate_match else "âŒ ä¸åŒ¹é…"
                print(f"    {service_name}: {cpu_anomaly_count} ä¸ªCPUå¼‚å¸¸ç‚¹ ({status})")
            else:
                print(f"    {service_name}: æ— CPUæ•°æ®")

        except Exception as e:
            print(f"    {service_name}: CPUæŸ¥è¯¢å¤±è´¥ - {e}")

    # æ­¥éª¤2ï¼šæ”¶é›†æ‰€æœ‰æœåŠ¡çš„å†…å­˜å¼‚å¸¸ä¿¡æ¯
    print(f"ğŸ” æ”¶é›†æ‰€æœ‰æœåŠ¡çš„å†…å­˜å¼‚å¸¸ä¿¡æ¯...")
    memory_anomalies = []

    for service_name, pattern_count in services_to_check:
        print(f"  æ£€æŸ¥ {service_name} çš„å†…å­˜å¼‚å¸¸...")

        try:
            memory_anomaly_query = f"""
.entity_set with(domain='k8s', name='k8s.deployment', query=`name='{service_name}'` )
| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')
| extend ret = series_decompose_anomalies(__value__, '{{"confidence": 0.035}}')
| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg
"""

            memory_result = cms_tester._execute_spl_query(
                memory_anomaly_query.strip(),
                from_time=from_time,
                to_time=to_time
            )

            if memory_result and memory_result.data:
                memory_anomaly_count = 0
                for record in memory_result.data:
                    if isinstance(record, (list, tuple)):
                        for item in record:
                            if isinstance(item, str):
                                exceed_upper_count = item.count('ExceedUpperBound')
                                memory_anomaly_count += exceed_upper_count

                memory_candidate = f"{service_name}.memory"
                is_candidate_match = memory_candidate in candidate_root_causes
                if not is_candidate_match:
                    is_candidate_match = service_name in memory_candidates
                memory_anomalies.append((service_name, memory_anomaly_count, is_candidate_match, memory_candidate))

                status = "âœ… åŒ¹é…" if is_candidate_match else "âŒ ä¸åŒ¹é…"
                print(f"    {service_name}: {memory_anomaly_count} ä¸ªå†…å­˜å¼‚å¸¸ç‚¹ ({status})")
            else:
                print(f"    {service_name}: æ— å†…å­˜æ•°æ®")

        except Exception as e:
            print(f"    {service_name}: å†…å­˜æŸ¥è¯¢å¤±è´¥ - {e}")

    # æ­¥éª¤3ï¼šåˆ†æCPUå¼‚å¸¸ï¼Œæ‰¾åˆ°æœ€é«˜çš„åŒ¹é…å€™é€‰
    print(f"ğŸ“Š åˆ†æCPUå¼‚å¸¸ç»“æœ...")
    cpu_anomalies.sort(key=lambda x: x[1], reverse=True)

    print(f"CPUå¼‚å¸¸æ’åºç»“æœ:")
    for service_name, anomaly_count, is_match, candidate in cpu_anomalies:
        status = "ğŸ¯ åŒ¹é…å€™é€‰" if is_match else "âŒ éå€™é€‰"
        print(f"  {service_name}: {anomaly_count} ç‚¹ ({status})")

    for service_name, anomaly_count, is_match, candidate in cpu_anomalies:
        if anomaly_count >= 3 and is_match:
            print(f"ğŸš¨ æ‰¾åˆ°CPUå¼‚å¸¸æ ¹å› : {candidate} (å¼‚å¸¸ç‚¹: {anomaly_count})")
            return [candidate]

    # æ­¥éª¤4ï¼šåˆ†æå†…å­˜å¼‚å¸¸ï¼Œæ‰¾åˆ°æœ€é«˜çš„åŒ¹é…å€™é€‰
    print(f"ğŸ“Š åˆ†æå†…å­˜å¼‚å¸¸ç»“æœ...")
    memory_anomalies.sort(key=lambda x: x[1], reverse=True)

    print(f"å†…å­˜å¼‚å¸¸æ’åºç»“æœ:")
    for service_name, anomaly_count, is_match, candidate in memory_anomalies:
        status = "ğŸ¯ åŒ¹é…å€™é€‰" if is_match else "âŒ éå€™é€‰"
        print(f"  {service_name}: {anomaly_count} ç‚¹ ({status})")

    for service_name, anomaly_count, is_match, candidate in memory_anomalies:
        if anomaly_count >= 3 and is_match:
            print(f"ğŸš¨ æ‰¾åˆ°å†…å­˜å¼‚å¸¸æ ¹å› : {candidate} (å¼‚å¸¸ç‚¹: {anomaly_count})")
            return [candidate]

    # æ­¥éª¤5ï¼šæ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å¼‚å¸¸
    print(f"âš ï¸ {analysis_type}æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ ¹å› ")
    return []


def analyze_latency_root_cause(anomaly_start_time: str, anomaly_end_time: str, candidate_root_causes: list):
    """
    Analyze latency root cause using comprehensive pattern and anomaly detection

    Args:
        anomaly_start_time: Start time of anomaly period (format: "YYYY-MM-DD HH:MM:SS")
        anomaly_end_time: End time of anomaly period (format: "YYYY-MM-DD HH:MM:SS")
        candidate_root_causes: List of candidate root causes to filter to

    Returns:
        list: Root cause candidate array (e.g., ["service.cpu"] or [])
    """

    # å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—å¹¶è¿›è¡Œå¼‚å¸¸å¤„ç†
    try:
        from find_root_cause_spans_rt import FindRootCauseSpansRT
        from test_cms_query import TestCMSQuery
        print("âœ… æˆåŠŸå¯¼å…¥è‡ªå®šä¹‰æ¨¡å—")
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
        print("è¯·ç¡®ä¿ç›¸å…³æ¨¡å—æ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•")
        return []

    print("ğŸš€ å¼€å§‹å»¶è¿Ÿæ ¹å› åˆ†æ...")
    print("="*60)

    print("ğŸ¯ Will analyze all services found in pattern analysis")

    # é…ç½®ä¿¡æ¯ - ä¿æŒåŸå§‹å€¼ä¸å˜
    PROJECT_NAME = "proj-xtrace-a46b97cfdc1332238f714864c014a1b-cn-qingdao"
    LOGSTORE_NAME = "logstore-tracing"
    REGION = "cn-qingdao"

    # åˆ†ææ—¶é—´åŒºé—´ - ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
    ANOMALY_START_TIME = anomaly_start_time
    ANOMALY_END_TIME = anomaly_end_time

    # è®¡ç®—æ­£å¸¸æ—¶é—´æ®µï¼ˆå¼‚å¸¸å¼€å§‹å‰1å°æ—¶åˆ°å¼‚å¸¸å¼€å§‹ï¼‰
    anomaly_start = datetime.strptime(ANOMALY_START_TIME, "%Y-%m-%d %H:%M:%S")
    normal_start = anomaly_start - timedelta(hours=1)
    normal_end = anomaly_start

    NORMAL_START_TIME = normal_start.strftime("%Y-%m-%d %H:%M:%S")
    NORMAL_END_TIME = normal_end.strftime("%Y-%m-%d %H:%M:%S")

    # CMSé…ç½® - ä¿æŒåŸå§‹å€¼ä¸å˜
    CMS_WORKSPACE = "quanxi-tianchi-test"
    CMS_ENDPOINT = 'metrics.cn-qingdao.aliyuncs.com'

    print(f"  SLS Project: {PROJECT_NAME}")
    print(f"  Logstore: {LOGSTORE_NAME}")
    print(f"  Region: {REGION}")
    print(f"  å¼‚å¸¸æ—¶é—´æ®µ: {ANOMALY_START_TIME} ~ {ANOMALY_END_TIME}")
    print(f"  æ­£å¸¸æ—¶é—´æ®µ: {NORMAL_START_TIME} ~ {NORMAL_END_TIME}")
    print(f"  CMS Workspace: {CMS_WORKSPACE}")

    # è·å–STSä¸´æ—¶å‡­è¯
    def get_sts_credentials():
        """è·å–STSä¸´æ—¶å‡­è¯"""
        try:
            from aliyunsdkcore.client import AcsClient
            from aliyunsdksts.request.v20150401 import AssumeRoleRequest

            # è·å–ç¯å¢ƒå˜é‡ä¸­çš„ä¸»è´¦å·å‡­è¯
            MAIN_ACCOUNT_ACCESS_KEY_ID = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
            MAIN_ACCOUNT_ACCESS_KEY_SECRET = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
            ALIBABA_CLOUD_ROLE_ARN = os.getenv('ALIBABA_CLOUD_ROLE_ARN', 'acs:ram::1672753017899339:role/tianchi-user-a')
            STS_SESSION_NAME = os.getenv('ALIBABA_CLOUD_ROLE_SESSION_NAME', 'my-sls-access')

            if not MAIN_ACCOUNT_ACCESS_KEY_ID or not MAIN_ACCOUNT_ACCESS_KEY_SECRET:
                print("âŒ ç¯å¢ƒå˜é‡æœªè®¾ç½®: ALIBABA_CLOUD_ACCESS_KEY_ID æˆ– ALIBABA_CLOUD_ACCESS_KEY_SECRET")
                return None, None, None

            client = AcsClient(
                MAIN_ACCOUNT_ACCESS_KEY_ID,
                MAIN_ACCOUNT_ACCESS_KEY_SECRET,
                REGION
            )

            request = AssumeRoleRequest.AssumeRoleRequest()
            request.set_RoleArn(ALIBABA_CLOUD_ROLE_ARN)
            request.set_RoleSessionName(STS_SESSION_NAME)
            request.set_DurationSeconds(3600)

            response = client.do_action_with_exception(request)
            response_data = json.loads(response)

            credentials = response_data['Credentials']
            return (
                credentials['AccessKeyId'],
                credentials['AccessKeySecret'],
                credentials['SecurityToken']
            )

        except Exception as e:
            print(f"âŒ è·å–STSä¸´æ—¶å‡­è¯å¤±è´¥: {e}")
            return None, None, None

    # è·å–ä¸´æ—¶å‡­è¯
    temp_access_key_id, temp_access_key_secret, security_token = get_sts_credentials()

    if not temp_access_key_id:
        print("âŒ æ— æ³•è·å–STSä¸´æ—¶å‡­è¯ï¼Œé€€å‡ºåˆ†æ")
        return []

    # åˆ›å»ºSLSå®¢æˆ·ç«¯
    try:
        from aliyun.log import LogClient

        sls_endpoint = f"{REGION}.log.aliyuncs.com"
        log_client = LogClient(
            sls_endpoint,
            temp_access_key_id,
            temp_access_key_secret,
            security_token
        )
        print("âœ… SLSå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")

    except Exception as e:
        print(f"âŒ åˆ›å»ºSLSå®¢æˆ·ç«¯å¤±è´¥: {e}")
        return []

    # åˆ›å»ºFindRootCauseSpansRTå®ä¾‹
    try:
        DURATION_THRESHOLD = 2000000000  # 2000msï¼ˆä»¥çº³ç§’ä¸ºå•ä½ï¼‰

        finder = FindRootCauseSpansRT(
            client=log_client,
            project_name=PROJECT_NAME,
            logstore_name=LOGSTORE_NAME,
            region=REGION,
            start_time=ANOMALY_START_TIME,
            end_time=ANOMALY_END_TIME,
            duration_threshold=DURATION_THRESHOLD,
            limit_num=1000,
            normal_start_time=NORMAL_START_TIME,
            normal_end_time=NORMAL_END_TIME,
            minus_average=True,
            only_top1_per_trace=False
        )
        print("âœ… FindRootCauseSpansRTå®ä¾‹åˆ›å»ºæˆåŠŸ")

    except Exception as e:
        print(f"âŒ åˆ›å»ºFindRootCauseSpansRTå®ä¾‹å¤±è´¥: {e}")
        return []

    # æ­¥éª¤1ï¼šæŸ¥æ‰¾é«˜ç‹¬å æ—¶é—´çš„span
    print("\nğŸ” Step 1: Finding high exclusive time spans...")
    print("="*60)

    try:
        top_95_percent_spans = finder.find_top_95_percent_spans()

        if top_95_percent_spans:
            print(f"âœ… æ‰¾åˆ° {len(top_95_percent_spans)} ä¸ªé«˜ç‹¬å æ—¶é—´çš„span")
            print(f"ğŸ“ å·²ç”Ÿæˆç”¨äºè¿›ä¸€æ­¥åˆ†æçš„æŸ¥è¯¢æ¡ä»¶")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°é«˜ç‹¬å æ—¶é—´çš„span")
            return []

    except Exception as e:
        print(f"âŒ æŸ¥æ‰¾é«˜ç‹¬å æ—¶é—´spanå¤±è´¥: {e}")
        return []

    # æ­¥éª¤2ï¼šä½¿ç”¨ diff_patterns è¿›è¡Œæ¨¡å¼åˆ†æ
    print("\nğŸ” Step 2: Pattern analysis with diff_patterns...")
    print("="*60)

    if top_95_percent_spans:
        # é¦–å…ˆå°†å…¨éƒ¨é«˜ç‹¬å æ—¶é—´çš„span_idæ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨äºdiff_patternsæŸ¥è¯¢æ¡ä»¶
        span_conditions_for_patterns = " or ".join([f"spanId='{span_id}'" for span_id in top_95_percent_spans[:2000]])  # Limit for query size

        param_str = """{"minimum_support_fraction": 0.03}"""
        # æ ¸å¿ƒä¸º diff_patterns ç®—æ³•è°ƒç”¨ï¼Œè¿›è¡Œæ¨¡å¼å·®å¼‚åˆ†æ
        diff_patterns_query = f"""
duration > {DURATION_THRESHOLD} | set session enable_remote_functions=true; set session velox_support_row_constructor_enabled=true;
with t0 as (
    select spanName, serviceName, cast(duration as double) as duration,
           JSON_EXTRACT_SCALAR(resources, '$["k8s.pod.ip"]') AS pod_ip,
           JSON_EXTRACT_SCALAR(resources, '$["k8s.node.name"]') AS node_name,
           JSON_EXTRACT_SCALAR(resources, '$["service.version"]') AS service_version,
           if(({span_conditions_for_patterns}), 'true', 'false') as anomaly_label,
           cast(if((statusCode = 2 or statusCode = 3), 1, 0) as double) as error_count
    from log
),
t1 as (
    select array_agg(spanName) as spanName,
           array_agg(serviceName) as serviceName,
           array_agg(duration) as duration,
           array_agg(pod_ip) as pod_ip,
           array_agg(node_name) as node_name,
           array_agg(service_version) as service_version,
           array_agg(anomaly_label) as anomaly_label,
           array_agg(error_count) as error_count
    from t0
),
t2 as (
    select row(spanName, serviceName, anomaly_label) as table_row
    from t1
),
t3 as (
    select diff_patterns(table_row, ARRAY['spanName', 'serviceName', 'anomaly_label'], 'anomaly_label', 'true', 'false', '', '', '{param_str}') as ret
    from t2
)
select * from t3
"""

        print("ğŸ“‹ å·²ç”Ÿæˆç”¨äºæ¨¡å¼åˆ†æçš„ diff_patterns æŸ¥è¯¢è¯­å¥")
        print("\nğŸš€ æ­£åœ¨ä½¿ç”¨SLSå®¢æˆ·ç«¯æ‰§è¡Œ diff_patterns æŸ¥è¯¢...")

        try:
            # åˆ›å»ºç”¨äº diff_patterns æŸ¥è¯¢çš„ GetLogsRequest
            from aliyun.log import GetLogsRequest

            request = GetLogsRequest(
                project=PROJECT_NAME,
                logstore=LOGSTORE_NAME,
                query=diff_patterns_query.strip(),
                fromTime=int(time.mktime(datetime.strptime(ANOMALY_START_TIME, "%Y-%m-%d %H:%M:%S").timetuple())),
                toTime=int(time.mktime(datetime.strptime(ANOMALY_END_TIME, "%Y-%m-%d %H:%M:%S").timetuple())),
                line=100  # Limit results
            )

            # ä½¿ç”¨finderçš„SLSå®¢æˆ·ç«¯æ‰§è¡ŒæŸ¥è¯¢
            patterns_result = log_client.get_logs(request)

            if patterns_result and patterns_result.get_logs():
                logs = [log_item.get_contents() for log_item in patterns_result.get_logs()]
                print(f"âœ… Pattern analysis completed: {len(logs)} results")

                # åœ¨ç»“æœä¸­ä¼šå±•ç¤ºåŒä¸€serviceåœ¨æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°å¯¹æ¯”
                print("\nğŸ“Š æ¨¡å¼åˆ†æç»“æœ:")
                for i, log_entry in enumerate(logs[:3]):  # Show first 3 results
                    print(f"  ç»“æœ {i+1}: {log_entry}")

                # Extract service patterns from diff_patterns results
                service_patterns = {}
                span_patterns = []  # Store span patterns for service inference

                for log_entry in logs:
                    if hasattr(log_entry, 'get_contents'):
                        contents = log_entry.get_contents()
                    else:
                        contents = log_entry
                    print(f"  ğŸ” Analyzing pattern result: {contents}")

                    # Parse structured result from diff_patterns query
                    if 'ret' in contents:
                        ret_value = contents['ret']

                        if isinstance(ret_value, str):
                            try:
                                # Replace 'null' with 'None' for Python parsing
                                data_str = ret_value.replace('null', 'None')
                                result = eval(data_str)

                                if len(result) >= 2 and isinstance(result[0], list) and isinstance(result[1], list):
                                    patterns = result[0]
                                    counts = result[1]

                                    print(f"    ğŸ“Š Found {len(patterns)} patterns with counts")

                                    for i, pattern in enumerate(patterns):
                                        if i < len(counts):
                                            count = counts[i]

                                            # Parse serviceName patterns from diff_patterns results
                                            if 'serviceName' in pattern and '=' in pattern:
                                                # Handle complex patterns like "serviceName"='cart' AND "spanName"='POST'
                                                import re
                                                match = re.search(r'"serviceName"=\'([^\']+)\'', pattern)
                                                if match:
                                                    service_part = match.group(1)
                                                    service_patterns[service_part] = service_patterns.get(service_part, 0) + count
                                                    print(f"    âœ… Found serviceName pattern: '{service_part}' (count: {count})")
                                                else:
                                                    print(f"    âš ï¸ Could not parse serviceName from: '{pattern}'")

                                            # Log spanName patterns for service inference
                                            elif 'spanName' in pattern:
                                                span_name = pattern.split('=')[1].strip('\'"') if '=' in pattern else pattern
                                                print(f"    â„¹ï¸ Found spanName pattern: '{span_name}' (count: {count})")
                                                span_patterns.append((span_name, count))

                            except Exception as e:
                                print(f"    âš ï¸ Error parsing ret field: {e}")
                                import traceback
                                traceback.print_exc()

                # If no serviceName patterns found, try to infer from spanName patterns
                if not service_patterns and span_patterns:
                    print(f"    ğŸ“Š No serviceName patterns found - attempting service inference from spanName patterns")

                    service_candidates = {}
                    for span_name, count in span_patterns:
                        # Map spanName patterns to likely services
                        if 'CartService' in span_name or 'cart' in span_name.lower():
                            service_candidates['cart'] = service_candidates.get('cart', 0) + count
                        elif 'ProductCatalogService' in span_name or 'product' in span_name.lower():
                            service_candidates['product-catalog'] = service_candidates.get('product-catalog', 0) + count
                        elif 'PaymentService' in span_name or 'payment' in span_name.lower():
                            service_candidates['payment'] = service_candidates.get('payment', 0) + count
                        elif 'CheckoutService' in span_name or 'checkout' in span_name.lower():
                            service_candidates['checkout'] = service_candidates.get('checkout', 0) + count
                        elif 'RecommendationService' in span_name or 'recommendation' in span_name.lower():
                            service_candidates['recommendation'] = service_candidates.get('recommendation', 0) + count
                        elif 'CurrencyService' in span_name or 'currency' in span_name.lower():
                            service_candidates['currency'] = service_candidates.get('currency', 0) + count
                        elif 'frontend' in span_name.lower():
                            service_candidates['frontend'] = service_candidates.get('frontend', 0) + count
                        elif 'flagservice' in span_name.lower() or 'ad' in span_name.lower():
                            service_candidates['ad'] = service_candidates.get('ad', 0) + count
                        else:
                            print(f"    â“ Cannot infer service from span: '{span_name}'")

                    if service_candidates:
                        print(f"    ğŸ“Š Service inference from spans: {dict(service_candidates)}")
                        service_patterns = service_candidates
                    else:
                        print("    âŒ Cannot determine target service from available span patterns")

                if service_patterns:
                    print(f"\nğŸ¯ è¯†åˆ«å‡ºçš„serviceæ¨¡å¼:")
                    for service, count in sorted(service_patterns.items(), key=lambda x: x[1], reverse=True):
                        print(f"  - {service}: {count} æ¬¡æ¨¡å¼åŒ¹é…")

                    # Sort all services by frequency (descending) for comprehensive analysis
                    all_service_matches = [(service, count) for service, count in service_patterns.items()]
                    all_service_matches.sort(key=lambda x: x[1], reverse=True)

                    print(f"\nğŸ’¡ æ‰€æœ‰æœåŠ¡æŒ‰æ¨¡å¼é¢‘ç‡æ’åº:")
                    for service, count in all_service_matches:
                        print(f"  - {service}: {count} æ¬¡æ¨¡å¼åŒ¹é…")

                    # Store all service matches for analysis
                    globals()['CANDIDATE_SERVICES_BY_FREQUENCY'] = all_service_matches

            else:
                print("âš ï¸ æœªè¿”å›ä»»ä½•æ¨¡å¼åˆ†æç»“æœ")

        except Exception as e:
            print(f"âŒ æ‰§è¡Œ diff_patterns æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
            print("ğŸ’¡ å»ºè®®åœ¨SLSæ§åˆ¶å°æ‰‹åŠ¨æ‰§è¡Œ")

    else:
        print("âš ï¸ æ— æ³•è¿›è¡Œæ¨¡å¼åˆ†æ - æœªæ‰¾åˆ°é«˜ç‹¬å æ—¶é—´çš„span")

    # æ­¥éª¤3ï¼šé¦–å…ˆå°è¯•æ¨¡å¼åˆ†ææœåŠ¡ï¼Œå¤±è´¥åˆ™å›é€€åˆ°æ‰€æœ‰å€™é€‰
    print("\nğŸ“Š Step 3: åˆ†æç­–ç•¥ - ä¼˜å…ˆæ¨¡å¼æœåŠ¡ï¼Œå¤±è´¥åˆ™å›é€€å…¨éƒ¨å€™é€‰")

    # å‡†å¤‡å€™é€‰æ ¹å› çš„CPUå’Œå†…å­˜åˆ†ç±»
    cpu_candidates = set()
    memory_candidates = set()

    for candidate in candidate_root_causes:
        if '.' in candidate:
            if candidate.endswith('.cpu'):
                service_name = candidate.split('.')[0]
                cpu_candidates.add(service_name)
            elif candidate.endswith('.memory'):
                service_name = candidate.split('.')[0]
                memory_candidates.add(service_name)

    # Store candidates for validation
    globals()['CPU_CANDIDATES'] = cpu_candidates
    globals()['MEMORY_CANDIDATES'] = memory_candidates

    print(f"ğŸ¯ è¾“å…¥å€™é€‰æœåŠ¡:")
    print(f"   CPU: {list(cpu_candidates)}")
    print(f"   å†…å­˜: {list(memory_candidates)}")

    # Create CMS client for the analysis
    try:
        cms_tester = TestCMSQuery()
        cms_tester.setUp()
        print(f"âœ… CMSå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")

    except Exception as e:
        print(f"âŒ åˆ›å»ºCMSå®¢æˆ·ç«¯å¤±è´¥: {e}")
        return []

    # Convert time strings to timestamps needed for CMS queries
    from_time = int(time.mktime(datetime.strptime(NORMAL_START_TIME, "%Y-%m-%d %H:%M:%S").timetuple()))
    to_time = int(time.mktime(datetime.strptime(ANOMALY_END_TIME, "%Y-%m-%d %H:%M:%S").timetuple()))

    # æ­¥éª¤3.1ï¼šé¦–å…ˆå°è¯•æ¨¡å¼åˆ†æå‘ç°çš„æœåŠ¡
    pattern_result = None
    if 'CANDIDATE_SERVICES_BY_FREQUENCY' in globals():
        pattern_services = globals()['CANDIDATE_SERVICES_BY_FREQUENCY']
        print(f"âœ… ä¼˜å…ˆåˆ†ææ¨¡å¼å‘ç°çš„æœåŠ¡: {[s[0] for s in pattern_services]}")

        pattern_result = analyze_services_for_anomalies(
            pattern_services, cpu_candidates, memory_candidates, candidate_root_causes,
            cms_tester, from_time, to_time, "æ¨¡å¼åˆ†æ"
        )

        if pattern_result:
            return pattern_result

    # æ­¥éª¤3.2ï¼šæ¨¡å¼åˆ†æå¤±è´¥æˆ–æ— ç»“æœï¼Œå›é€€åˆ°æ‰€æœ‰è¾“å…¥å€™é€‰
    print("\nğŸ”„ æ¨¡å¼åˆ†ææœªæ‰¾åˆ°æ ¹å› ï¼Œå›é€€åˆ°åˆ†ææ‰€æœ‰è¾“å…¥å€™é€‰...")
    all_input_services = cpu_candidates.union(memory_candidates)
    if all_input_services:
        fallback_services = [(service, 1) for service in all_input_services]
        print(f"ğŸš€ å›é€€åˆ†ææœåŠ¡: {[s[0] for s in fallback_services]}")

        fallback_result = analyze_services_for_anomalies(
            fallback_services, cpu_candidates, memory_candidates, candidate_root_causes,
            cms_tester, from_time, to_time, "å›é€€åˆ†æ"
        )

        return fallback_result if fallback_result else []
    else:
        print("âŒ è¾“å…¥å€™é€‰ä¸­æ²¡æœ‰æœ‰æ•ˆçš„æœåŠ¡æ ¼å¼")
        return []
