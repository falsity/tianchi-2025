{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPL è¯Šæ–­é—®é¢˜æ ¹å› -æ…¢\n",
    "\n",
    "è¯¥notebookå®ç°äº†ã€Šå¦‚ä½•ç”¨SPLå¿«é€Ÿè¯Šæ–­é—®é¢˜æ ¹å›  -- æ…¢.mdã€‹ä¸­æè¿°çš„é€æ­¥æµç¨‹ï¼Œç”¨äºé€šè¿‡SPLæŸ¥è¯¢è¯Šæ–­ç«¯åˆ°ç«¯é«˜å»¶è¿Ÿé—®é¢˜ã€‚\n",
    "\n",
    "## åˆ†ææµç¨‹\n",
    "\n",
    "1. æŸ¥æ‰¾é«˜ç‹¬å æ—¶é—´çš„span - ä½¿ç”¨ trace_exclusive_duration è¿ç®—ç¬¦è¯†åˆ«ç‹¬å æŒç»­æ—¶é—´é«˜çš„span\n",
    "2. æ¨¡å¼åˆ†æ - ä½¿ç”¨ diff_patterns å‘ç°é«˜å»¶è¿Ÿspançš„ç‰¹å¾\n",
    "3. CPUæŒ‡æ ‡åˆ†æ - é€šè¿‡CMSå®ä½“æŸ¥è¯¢ï¼ŒæŸ¥è¯¢æŒ‡å®šserviceçš„CPUä½¿ç”¨ç‡æŒ‡æ ‡\n",
    "4. å¼‚å¸¸æ£€æµ‹ - ä½¿ç”¨ series_decompose_anomalies æ£€æµ‹CPUä½¿ç”¨ç‡å¼‚å¸¸\n",
    "\n",
    "ç›®æ ‡æ˜¯æ‰¾å‡ºèƒ½å¤Ÿè§£é‡Šå»¶è¿Ÿä¸Šå‡çš„root cause ï¼ˆä¾‹å¦‚ recommendation.cpuï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FindRootCauseSpansRT imported successfully\n",
      "âœ… TestCMSQuery imported successfully\n",
      "âœ… Constants imported successfully\n",
      "âœ… All available imports loaded successfully\n",
      "\n",
      "ğŸ’¡ If you see warnings above, please run: pip install -r requirements.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# å°†çˆ¶ç›®å½•æ·»åŠ åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥æ¨¡å—\n",
    "sys.path.append('..')\n",
    "\n",
    "# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—å¹¶è¿›è¡Œå¼‚å¸¸å¤„ç†\n",
    "try:\n",
    "    from find_root_cause_spans_rt import FindRootCauseSpansRT\n",
    "    print(\"âœ… FindRootCauseSpansRT imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Warning: Could not import FindRootCauseSpansRT: {e}\")\n",
    "    FindRootCauseSpansRT = None\n",
    "\n",
    "try:\n",
    "    from test_cms_query import TestCMSQuery\n",
    "    print(\"âœ… TestCMSQuery imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Warning: Could not import TestCMSQuery: {e}\")\n",
    "    print(\"ğŸ’¡ Please install required dependencies: pip install -r requirements.txt\")\n",
    "    TestCMSQuery = None\n",
    "\n",
    "try:\n",
    "    from utils.constants import HIGH_RT_TRACES, TRACES_FOR_AVG_RT, PERCENT_95\n",
    "    print(\"âœ… Constants imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Warning: Could not import constants: {e}\")\n",
    "    # å¦‚æœå¸¸é‡æ— æ³•å¯¼å…¥ï¼Œåˆ™è®¾ç½®é»˜è®¤å€¼\n",
    "    HIGH_RT_TRACES = 1000\n",
    "    TRACES_FOR_AVG_RT = 1000\n",
    "    PERCENT_95 = 0.95\n",
    "    print(\"âœ… Using default constant values\")\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "\n",
    "print(\"âœ… All available imports loaded successfully\")\n",
    "print(\"\\nğŸ’¡ If you see warnings above, please run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é…ç½®ç¯å¢ƒå˜é‡\n",
    "\n",
    "è®¾ç½®è®¿é—®çš„projectå’Œlogstore,è®¾ç½®é¢˜ç›®ä¸­çš„æ•…éšœæ—¶é—´æ®µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Analysis Configuration:\n",
      "  SLS Project: proj-xtrace-a46b97cfdc1332238f714864c014a1b-cn-qingdao\n",
      "  Logstore: logstore-tracing\n",
      "  Anomaly Period: 2025-08-28 16:14:30 to 2025-08-28 16:19:30\n",
      "  Normal Period: 2025-08-28 16:04:30 to 2025-08-28 16:14:30\n",
      "  Duration Threshold: 2.0s\n",
      "  CMS Workspace: quanxi-tianchi-test\n"
     ]
    }
   ],
   "source": [
    "# SLS é…ç½®\n",
    "PROJECT_NAME = \"proj-xtrace-a46b97cfdc1332238f714864c014a1b-cn-qingdao\"\n",
    "LOGSTORE_NAME = \"logstore-tracing\"\n",
    "REGION = \"cn-qingdao\"\n",
    "\n",
    "# åˆ†ææ—¶é—´åŒºé—´\n",
    "# å¼‚å¸¸æ—¶é—´æ®µï¼ˆå»¶è¿Ÿè¾ƒé«˜æ—¶æ®µï¼‰\n",
    "ANOMALY_START_TIME = \"2025-08-28 16:14:30\"\n",
    "ANOMALY_END_TIME = \"2025-08-28 16:19:30\"\n",
    "\n",
    "# æ­£å¸¸æ—¶é—´æ®µï¼ˆç”¨äºå¯¹æ¯”çš„åŸºçº¿ï¼‰\n",
    "NORMAL_START_TIME = \"2025-08-28 16:04:30\"\n",
    "NORMAL_END_TIME = \"2025-08-28 16:14:30\"\n",
    "\n",
    "# åˆ†æå‚æ•°\n",
    "DURATION_THRESHOLD = 2000000000  # 2000msï¼ˆä»¥çº³ç§’ä¸ºå•ä½ï¼‰\n",
    "LIMIT_NUM = 1000  # åˆ†æçš„ trace æ•°é‡\n",
    "\n",
    "# CMS æŒ‡æ ‡é…ç½®\n",
    "CMS_WORKSPACE = \"quanxi-tianchi-test\"\n",
    "CMS_ENDPOINT = 'cms.cn-qingdao.aliyuncs.com'\n",
    "\n",
    "print(f\"ğŸ“Š Analysis Configuration:\")\n",
    "print(f\"  SLS Project: {PROJECT_NAME}\")\n",
    "print(f\"  Logstore: {LOGSTORE_NAME}\")\n",
    "print(f\"  Anomaly Period: {ANOMALY_START_TIME} to {ANOMALY_END_TIME}\")\n",
    "print(f\"  Normal Period: {NORMAL_START_TIME} to {NORMAL_END_TIME}\")\n",
    "print(f\"  Duration Threshold: {DURATION_THRESHOLD/1000000000:.1f}s\")\n",
    "print(f\"  CMS Workspace: {CMS_WORKSPACE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STSåˆ›å»ºå®¢æˆ·ç«¯\n",
    "\n",
    "è®¾ç½®ä¸ªäººè´¦æˆ·ä¿¡æ¯ï¼Œè·å–è®¿é—®æƒé™ï¼ŒSTSæ–°å»ºå®¢æˆ·ç«¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SLSè®¿é—®å‡­è¯é…ç½®æ­£ç¡®\n",
      "âœ… æˆåŠŸè·å–è®¿é—®æƒé™ï¼\n",
      "âœ… SLSå®¢æˆ·ç«¯å·²ä½¿ç”¨ä¸´æ—¶å‡­è¯åˆ›å»ºã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from aliyun.log import LogClient \n",
    "from alibabacloud_sts20150401.client import Client as StsClient\n",
    "from alibabacloud_sts20150401 import models as sts_models\n",
    "from alibabacloud_tea_openapi import models as open_api_models\n",
    "from Tea.exceptions import TeaException\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "# ----------è¯·åˆ›å»ºä¸€ä¸ªç¯å¢ƒå˜é‡æ–‡ä»¶,å¹¶å°†ä¸‹é¢2ä¸ªå‚æ•°è®¾ç½®ä¸ºä½ ä¹‹å‰ä¿å­˜çš„ä¿¡æ¯\n",
    "# ----------è¯·åœ¨ç¯å¢ƒå˜é‡æ–‡ä»¶ä¸­è®¾ç½®ä½ åˆ›å»ºç”¨æˆ·æ—¶ä¿å­˜çš„AK, æ ·ä¾‹å¦‚ä¸‹:\n",
    "'''\n",
    "ALIBABA_CLOUD_ACCESS_KEY_ID=\"ä½ ä¿å­˜çš„AccessKey ID\"\n",
    "ALIBABA_CLOUD_ACCESS_KEY_SECRET\"ä½ ä¿å­˜çš„AccessKey Secret\"\n",
    "'''\n",
    "MAIN_ACCOUNT_ACCESS_KEY_ID = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')\n",
    "MAIN_ACCOUNT_ACCESS_KEY_SECRET = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')\n",
    "ALIBABA_CLOUD_ROLE_ARN = os.getenv('ALIBABA_CLOUD_ROLE_ARN','acs:ram::1672753017899339:role/tianchi-user-a')\n",
    "STS_SESSION_NAME = os.getenv('ALIBABA_CLOUD_ROLE_SESSION_NAME', 'my-sls-access') # è‡ªå®šä¹‰ä¼šè¯åç§°ï¼Œæ²¡æœ‰å›ºå®šè¦æ±‚\n",
    "\n",
    "if MAIN_ACCOUNT_ACCESS_KEY_ID and MAIN_ACCOUNT_ACCESS_KEY_SECRET and ALIBABA_CLOUD_ROLE_ARN:\n",
    "    print(\"âœ… SLSè®¿é—®å‡­è¯é…ç½®æ­£ç¡®\")\n",
    "else:\n",
    "    print(\"âŒ è¯·åœ¨ç¯å¢ƒå˜é‡æ–‡ä»¶ä¸­é…ç½®ALIBABA_CLOUD_ACCESS_KEY_IDå’ŒALIBABA_CLOUD_ACCESS_KEY_SECRET\")\n",
    "\n",
    "\n",
    "def get_sts_credentials():\n",
    "    \n",
    "    if not all([MAIN_ACCOUNT_ACCESS_KEY_ID, MAIN_ACCOUNT_ACCESS_KEY_SECRET, ALIBABA_CLOUD_ROLE_ARN]):\n",
    "        print(\"âŒ ä¸ªäººè´¦å·ä¿¡æ¯ç¼ºå¤±! è¯·åœ¨ç¯å¢ƒå˜é‡æ–‡ä»¶ä¸­é…ç½® ALIBABA_CLOUD_ACCESS_KEY_ID, ALIBABA_CLOUD_ACCESS_KEY_SECRET\")\n",
    "        return None\n",
    "\n",
    "    config = open_api_models.Config(\n",
    "        access_key_id=MAIN_ACCOUNT_ACCESS_KEY_ID, # type: ignore\n",
    "        access_key_secret=MAIN_ACCOUNT_ACCESS_KEY_SECRET, # type: ignore\n",
    "        endpoint=f'sts.{REGION}.aliyuncs.com'\n",
    "    )\n",
    "    sts_client = StsClient(config)\n",
    "    \n",
    "    assume_role_request = sts_models.AssumeRoleRequest(\n",
    "        role_arn=ALIBABA_CLOUD_ROLE_ARN, # type: ignore\n",
    "        role_session_name=STS_SESSION_NAME,\n",
    "        duration_seconds=3600\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = sts_client.assume_role(assume_role_request)\n",
    "        print(\"âœ… æˆåŠŸè·å–è®¿é—®æƒé™ï¼\")\n",
    "        return response.body.credentials\n",
    "    except TeaException as e:\n",
    "        print(f\"âŒ è·å–STSä¸´æ—¶å‡­è¯å¤±è´¥: {e.message}\")\n",
    "        print(f\"  é”™è¯¯ç : {e.code}\")\n",
    "        print(\"  è¯·æ£€æŸ¥:1. ä¸»è´¦å·AKæ˜¯å¦æ­£ç¡®;2. ç›®æ ‡è§’è‰²ARNæ˜¯å¦æ­£ç¡®;3. ç›®æ ‡è§’è‰²çš„ä¿¡ä»»ç­–ç•¥æ˜¯å¦å·²é…ç½®ä¸ºä¿¡ä»»æ‚¨çš„ä¸»è´¦å·ã€‚\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å‘ç”ŸæœªçŸ¥é”™è¯¯åœ¨è·å–STSå‡­è¯æ—¶: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- å‡½æ•°ï¼šåˆ›å»ºSLSå®¢æˆ·ç«¯ ---\n",
    "def create_sls_client_with_sts():\n",
    "    \n",
    "    sts_credentials = get_sts_credentials()\n",
    "    \n",
    "    if not sts_credentials:\n",
    "        return None\n",
    "        \n",
    "    sls_endpoint = f\"{REGION}.log.aliyuncs.com\"\n",
    "    \n",
    "    # aliyun-log-python-sdk ä½¿ç”¨ securityToken å‚æ•°\n",
    "    log_client = LogClient(\n",
    "        endpoint=sls_endpoint,\n",
    "        accessKeyId=sts_credentials.access_key_id,\n",
    "        accessKey=sts_credentials.access_key_secret,\n",
    "        securityToken=sts_credentials.security_token  \n",
    "    )\n",
    "    \n",
    "    print(\"âœ… SLSå®¢æˆ·ç«¯å·²ä½¿ç”¨ä¸´æ—¶å‡­è¯åˆ›å»ºã€‚\")\n",
    "    return log_client\n",
    "\n",
    "# åˆ›å»ºå¸¦æœ‰STSå‡­è¯çš„SLSå®¢æˆ·ç«¯\n",
    "log_client_instance = create_sls_client_with_sts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤1ï¼šæŸ¥æ‰¾é«˜ç‹¬å æ—¶é—´çš„Span\n",
    "\n",
    "ä½¿ç”¨ trace_exclusive_duration è¿ç®—ç¬¦ï¼Œåœ¨å¼‚å¸¸æ—¶æ®µè¯†åˆ«ç‹¬å æ—¶é—´é«˜çš„spanã€‚\n",
    "\n",
    "ç‹¬å æ—¶é—´ï¼šspanå†…å®é™…æ¶ˆè€—çš„æ—¶é—´ï¼Œä¸åŒ…æ‹¬å…¶å­spanæ¶ˆè€—çš„æ—¶é—´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Step 1: Finding high exclusive time spans...\n",
      "============================================================\n",
      "è·å–ç‹¬å æ—¶é—´æ•°æ®...\n",
      "æ­£å¸¸æ—¶é—´æ®µæŸ¥è¯¢åˆ°çš„ç‹¬å æ—¶é—´æ—¥å¿—æ¡æ•°: 3000\n",
      "å¼€å§‹è®¡ç®—æ­£å¸¸æ—¶é—´æ®µçš„å¹³å‡ç‹¬å æ—¶é—´...\n",
      "æ”¶é›†åˆ° 13281 ä¸ªspançš„ç‹¬å æ—¶é—´ä¿¡æ¯\n",
      "æŸ¥è¯¢spançš„serviceNameå’ŒspanNameä¿¡æ¯...\n",
      "ä»åŸå§‹spanä¸­é‡‡æ ·äº† 3000 ä¸ªç”¨äºè®¡ç®—å¹³å‡å€¼\n",
      "æŸ¥è¯¢ç¬¬ 1 æ‰¹ï¼Œå…± 500 ä¸ªspan...\n",
      "æŸ¥è¯¢åˆ° 100 æ¡è®°å½•\n",
      "æŸ¥è¯¢ç¬¬ 2 æ‰¹ï¼Œå…± 500 ä¸ªspan...\n",
      "æŸ¥è¯¢åˆ° 100 æ¡è®°å½•\n",
      "æŸ¥è¯¢ç¬¬ 3 æ‰¹ï¼Œå…± 500 ä¸ªspan...\n",
      "æŸ¥è¯¢åˆ° 100 æ¡è®°å½•\n",
      "æŸ¥è¯¢ç¬¬ 4 æ‰¹ï¼Œå…± 500 ä¸ªspan...\n",
      "æŸ¥è¯¢åˆ° 100 æ¡è®°å½•\n",
      "æŸ¥è¯¢ç¬¬ 5 æ‰¹ï¼Œå…± 500 ä¸ªspan...\n",
      "æŸ¥è¯¢åˆ° 100 æ¡è®°å½•\n",
      "æŸ¥è¯¢ç¬¬ 6 æ‰¹ï¼Œå…± 500 ä¸ªspan...\n",
      "æŸ¥è¯¢åˆ° 100 æ¡è®°å½•\n",
      "ç»„åˆé”® frontend-proxy<sep>router flagservice egress çš„å¹³å‡ç‹¬å æ—¶é—´: 2105060777.78\n",
      "ç»„åˆé”® frontend-web<sep>resourceFetch çš„å¹³å‡ç‹¬å æ—¶é—´: 131874850.34\n",
      "ç»„åˆé”® frontend-web<sep>documentLoad çš„å¹³å‡ç‹¬å æ—¶é—´: 367325072.75\n",
      "ç»„åˆé”® frontend<sep>GET çš„å¹³å‡ç‹¬å æ—¶é—´: 13495926.74\n",
      "ç»„åˆé”® frontend-web<sep>HTTP GET çš„å¹³å‡ç‹¬å æ—¶é—´: 72617073.17\n",
      "ç»„åˆé”® frontend<sep>grpc.oteldemo.RecommendationService/ListRecommendations çš„å¹³å‡ç‹¬å æ—¶é—´: 61212609.08\n",
      "ç»„åˆé”® recommendation<sep>get_product_list çš„å¹³å‡ç‹¬å æ—¶é—´: 39862021.38\n",
      "ç»„åˆé”® frontend-web<sep>HTTP POST çš„å¹³å‡ç‹¬å æ—¶é—´: 113213333.33\n",
      "ç»„åˆé”® frontend-web<sep>documentFetch çš„å¹³å‡ç‹¬å æ—¶é—´: 32211111.33\n",
      "ç»„åˆé”® frontend-proxy<sep>router frontend egress çš„å¹³å‡ç‹¬å æ—¶é—´: 22540037.34\n",
      "ç»„åˆé”® recommendation<sep>/oteldemo.RecommendationService/ListRecommendations çš„å¹³å‡ç‹¬å æ—¶é—´: 21925908.81\n",
      "ç»„åˆé”® recommendation<sep>/oteldemo.ProductCatalogService/ListProducts çš„å¹³å‡ç‹¬å æ—¶é—´: 14892128.67\n",
      "ç»„åˆé”® load-generator<sep>POST çš„å¹³å‡ç‹¬å æ—¶é—´: 5148330.43\n",
      "ç»„åˆé”® frontend<sep>grpc.oteldemo.CartService/GetCart çš„å¹³å‡ç‹¬å æ—¶é—´: 7286159.67\n",
      "ç»„åˆé”® frontend<sep>render route (pages) /cart çš„å¹³å‡ç‹¬å æ—¶é—´: 8715785.00\n",
      "ç»„åˆé”® frontend<sep>GET /api/recommendations çš„å¹³å‡ç‹¬å æ—¶é—´: 15000000.00\n",
      "ç»„åˆé”® frontend<sep>grpc.oteldemo.ProductCatalogService/GetProduct çš„å¹³å‡ç‹¬å æ—¶é—´: 3807425.70\n",
      "ç»„åˆé”® frontend<sep>grpc.oteldemo.CurrencyService/Convert çš„å¹³å‡ç‹¬å æ—¶é—´: 4767074.38\n",
      "ç»„åˆé”® cart<sep>POST /oteldemo.CartService/GetCart çš„å¹³å‡ç‹¬å æ—¶é—´: 4717800.00\n",
      "ç»„åˆé”® frontend-proxy<sep>router image-provider egress çš„å¹³å‡ç‹¬å æ—¶é—´: 3769000.00\n",
      "ç»„åˆé”® currency<sep>Currency/Convert çš„å¹³å‡ç‹¬å æ—¶é—´: 4539705.31\n",
      "ç»„åˆé”® cart<sep>POST çš„å¹³å‡ç‹¬å æ—¶é—´: 4478640.00\n",
      "ç»„åˆé”® cart<sep>HGET çš„å¹³å‡ç‹¬å æ—¶é—´: 4631800.00\n",
      "ç»„åˆé”® currency<sep>Currency/GetSupportedCurrencies çš„å¹³å‡ç‹¬å æ—¶é—´: 3594947.00\n",
      "ç»„åˆé”® frontend<sep>grpc.oteldemo.ProductCatalogService/ListProducts çš„å¹³å‡ç‹¬å æ—¶é—´: 3847082.25\n",
      "ç»„åˆé”® frontend<sep>grpc.oteldemo.CurrencyService/GetSupportedCurrencies çš„å¹³å‡ç‹¬å æ—¶é—´: 3188118.17\n",
      "ç»„åˆé”® product-catalog<sep>oteldemo.ProductCatalogService/GetProduct çš„å¹³å‡ç‹¬å æ—¶é—´: 3092606.44\n",
      "ç»„åˆé”® frontend<sep>render route (pages) / çš„å¹³å‡ç‹¬å æ—¶é—´: 3115563.62\n",
      "ç»„åˆé”® product-catalog<sep>oteldemo.ProductCatalogService/ListProducts çš„å¹³å‡ç‹¬å æ—¶é—´: 2931132.00\n",
      "ç»„åˆé”® frontend<sep>GET /cart çš„å¹³å‡ç‹¬å æ—¶é—´: 2872127.00\n",
      "ç»„åˆé”® cart<sep>POST /oteldemo.CartService/EmptyCart çš„å¹³å‡ç‹¬å æ—¶é—´: 2568200.00\n",
      "ç»„åˆé”® frontend<sep>GET / çš„å¹³å‡ç‹¬å æ—¶é—´: 2896553.00\n",
      "å…±è®¡ç®—äº† 32 ä¸ªserviceName<sep>spanNameç»„åˆçš„å¹³å‡ç‹¬å æ—¶é—´\n",
      "âš™ï¸  Analyzer å·²åˆå§‹åŒ–ï¼Œå¹¶å¯ç”¨äº†åŸºçº¿å¯¹æ¯”åŠŸèƒ½\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Step 1: Finding high exclusive time spans...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from find_root_cause_spans_rt import FindRootCauseSpansRT\n",
    "\n",
    "# ä½¿ç”¨åŸºçº¿å¯¹æ¯”æ–¹å¼åˆå§‹åŒ–æ ¹å› åˆ†æå™¨\n",
    "finder = FindRootCauseSpansRT(\n",
    "    client=log_client_instance,\n",
    "    project_name=PROJECT_NAME,\n",
    "    logstore_name=LOGSTORE_NAME,\n",
    "    region=REGION,\n",
    "    start_time=ANOMALY_START_TIME,\n",
    "    end_time=ANOMALY_END_TIME,\n",
    "    duration_threshold=DURATION_THRESHOLD,\n",
    "    limit_num=LIMIT_NUM,\n",
    "    normal_start_time=NORMAL_START_TIME,\n",
    "    normal_end_time=NORMAL_END_TIME,\n",
    "    minus_average=True,  # å¯ç”¨åŸºçº¿å€¼ç›¸å‡ï¼Œæœ‰åŠ©äºæ›´å¥½åœ°æ£€æµ‹å¼‚å¸¸\n",
    "    only_top1_per_trace=False\n",
    ") # type: ignore\n",
    "\n",
    "print(\"âš™ï¸  Analyzer å·²åˆå§‹åŒ–ï¼Œå¹¶å¯ç”¨äº†åŸºçº¿å¯¹æ¯”åŠŸèƒ½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ˆ æœ€å¤šåˆ†æ 1000 æ¡æŒç»­æ—¶é—´å¤§äº 2.0 ç§’çš„ trace\n",
      "ğŸ¯ æ­£åœ¨æŸ¥æ‰¾è´¡çŒ®ç‹¬å æ—¶é—´å‰95%çš„span...\n",
      "æŸ¥è¯¢åˆ°çš„æ—¥å¿—æ¡æ•°: 2000\n",
      "ğŸ”§ å¤„ç†æ¨¡å¼: å¤„ç†æ¯ä¸ªtraceä¸­çš„æ‰€æœ‰span\n",
      "æ€»å…±æ‰¾åˆ° 4115 ä¸ªæœ‰æ•ˆçš„spanç‹¬å æ—¶é—´æ•°æ®\n",
      "æˆåŠŸæ˜ å°„ 2119 ä¸ªspançš„serviceNameå’ŒspanName\n",
      "æ–¹æ¡ˆ1è¦†ç›–ç‡: 51.49% (2119/4115)\n",
      "âœ… é€‰æ‹©æ–¹æ¡ˆ1ï¼šç›´æ¥ä½¿ç”¨span_listä¸­çš„serviceNameå’ŒspanNameï¼ˆæ¨èï¼‰\n",
      "ğŸš€ [æ–¹æ¡ˆ1] ä½¿ç”¨span_listä¸­çš„serviceNameå’ŒspanNameè¿›è¡Œè°ƒæ•´...\n",
      "ğŸš€ [æ–¹æ¡ˆ1] æ— éœ€é¢å¤–æŸ¥è¯¢ï¼Œç›´æ¥å¤„ç† 4115 ä¸ªspan\n",
      "å¼€å§‹æœ¬åœ°è®¡ç®—è°ƒæ•´åçš„ç‹¬å æ—¶é—´...\n",
      "å®Œæˆ 4115 ä¸ªspançš„æ—¶é—´è°ƒæ•´è®¡ç®—\n",
      "æ€»ç‹¬å æ—¶é—´: 10094340059\n",
      "å å‰95%ç‹¬å æ—¶é—´çš„spanæ•°é‡: 1918\n",
      "è¿™äº›spançš„ç´¯è®¡ç‹¬å æ—¶é—´: 9590000000, å æ€»æ—¶é—´çš„: 95.00%\n",
      "\n",
      "ğŸ“Š ç»“æœ:\n",
      "  å…±æ‰¾åˆ° 1918 ä¸ªè´¡çŒ®äº†ç‹¬å æ—¶é—´å‰95%çš„span\n",
      "\n",
      "ğŸ” ç¤ºä¾‹span IDs:\n",
      "  1. 84f6544f9e2a9f46\n",
      "  2. 481f04f735a798f9\n",
      "  3. ac75fde1026dbf0b\n",
      "  4. e5acfd096fb6acac\n",
      "  5. 0dc22ae39727061c\n",
      "  ... and 1913 more spans\n",
      "æŸ¥è¯¢åˆ°çš„æ—¥å¿—æ¡æ•°: 2000\n",
      "ğŸ”§ å¤„ç†æ¨¡å¼: å¤„ç†æ¯ä¸ªtraceä¸­çš„æ‰€æœ‰span\n",
      "æ€»å…±æ‰¾åˆ° 4115 ä¸ªæœ‰æ•ˆçš„spanç‹¬å æ—¶é—´æ•°æ®\n",
      "æˆåŠŸæ˜ å°„ 2119 ä¸ªspançš„serviceNameå’ŒspanName\n",
      "æ–¹æ¡ˆ1è¦†ç›–ç‡: 51.49% (2119/4115)\n",
      "âœ… é€‰æ‹©æ–¹æ¡ˆ1ï¼šç›´æ¥ä½¿ç”¨span_listä¸­çš„serviceNameå’ŒspanNameï¼ˆæ¨èï¼‰\n",
      "ğŸš€ [æ–¹æ¡ˆ1] ä½¿ç”¨span_listä¸­çš„serviceNameå’ŒspanNameè¿›è¡Œè°ƒæ•´...\n",
      "ğŸš€ [æ–¹æ¡ˆ1] æ— éœ€é¢å¤–æŸ¥è¯¢ï¼Œç›´æ¥å¤„ç† 4115 ä¸ªspan\n",
      "å¼€å§‹æœ¬åœ°è®¡ç®—è°ƒæ•´åçš„ç‹¬å æ—¶é—´...\n",
      "å®Œæˆ 4115 ä¸ªspançš„æ—¶é—´è°ƒæ•´è®¡ç®—\n",
      "æ€»ç‹¬å æ—¶é—´: 10094340059\n",
      "å å‰95%ç‹¬å æ—¶é—´çš„spanæ•°é‡: 1918\n",
      "è¿™äº›spançš„ç´¯è®¡ç‹¬å æ—¶é—´: 9590000000, å æ€»æ—¶é—´çš„: 95.00%\n",
      "\n",
      "ğŸ“ å·²ç”Ÿæˆç”¨äºè¿›ä¸€æ­¥åˆ†æçš„æŸ¥è¯¢æ¡ä»¶\n"
     ]
    }
   ],
   "source": [
    "print(f\"ğŸ“ˆ æœ€å¤šåˆ†æ {LIMIT_NUM} æ¡æŒç»­æ—¶é—´å¤§äº {DURATION_THRESHOLD/1000000000:.1f} ç§’çš„ trace\")\n",
    "\n",
    "# æŸ¥æ‰¾è´¡çŒ®äº†ç‹¬å æ—¶é—´å‰95%çš„span\n",
    "print(\"ğŸ¯ æ­£åœ¨æŸ¥æ‰¾è´¡çŒ®ç‹¬å æ—¶é—´å‰95%çš„span...\")\n",
    "\n",
    "top_95_percent_spans = finder.find_top_95_percent_spans()\n",
    "\n",
    "print(f\"\\nğŸ“Š ç»“æœ:\")\n",
    "print(f\"  å…±æ‰¾åˆ° {len(top_95_percent_spans)} ä¸ªè´¡çŒ®äº†ç‹¬å æ—¶é—´å‰95%çš„span\")\n",
    "\n",
    "if top_95_percent_spans:\n",
    "    print(f\"\\nğŸ” ç¤ºä¾‹span IDs:\")\n",
    "    for i, span_id in enumerate(top_95_percent_spans[:5]):\n",
    "        print(f\"  {i+1}. {span_id}\")\n",
    "    \n",
    "    if len(top_95_percent_spans) > 5:\n",
    "        print(f\"  ... and {len(top_95_percent_spans) - 5} more spans\")\n",
    "        \n",
    "    # Get the query conditions for these spans\n",
    "    span_conditions, detailed_query = finder.get_top_95_percent_spans_query()\n",
    "    print(f\"\\nğŸ“ å·²ç”Ÿæˆç”¨äºè¿›ä¸€æ­¥åˆ†æçš„æŸ¥è¯¢æ¡ä»¶\")\n",
    "else:\n",
    "    print(\"âš ï¸  æœªæ‰¾åˆ°é«˜ç‹¬å æ—¶é—´çš„span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤2ï¼šä½¿ç”¨ diff_patterns è¿›è¡Œæ¨¡å¼åˆ†æ\n",
    "\n",
    "ä½¿ç”¨ diff_patterns è¿ç®—ç¬¦ï¼Œå‘ç°é«˜ç‹¬å æ—¶é—´spanä¸æ­£å¸¸spançš„åŒºåˆ«ç‰¹å¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Step 2: Pattern analysis with diff_patterns...\n",
      "============================================================\n",
      "ğŸ“‹ å·²ç”Ÿæˆç”¨äºæ¨¡å¼åˆ†æçš„ diff_patterns æŸ¥è¯¢è¯­å¥\n",
      "\n",
      "ğŸš€ æ­£åœ¨ä½¿ç”¨SLSå®¢æˆ·ç«¯æ‰§è¡Œ diff_patterns æŸ¥è¯¢...\n",
      "âœ… æ¨¡å¼åˆ†æå®Œæˆï¼šå…±è¿”å› 1 æ¡ç»“æœè®°å½•\n",
      "\n",
      "ğŸ“Š æ¨¡å¼åˆ†æç»“æœ:\n",
      "  ç»“æœ 1: {'ret': '[[\"\\\\\"spanName\\\\\"=\\'router flagservice egress\\'\"],[1869],[245],[0.9744525547445255],[0.09729944400317713],[0.8771531107413484],[1.0],[0.0],null]'}\n",
      "  ğŸ” Analyzing pattern result: {'ret': '[[\"\\\\\"spanName\\\\\"=\\'router flagservice egress\\'\"],[1869],[245],[0.9744525547445255],[0.09729944400317713],[0.8771531107413484],[1.0],[0.0],null]'}\n",
      "    ğŸ“Š Extracted patterns: ['\"spanName\"=\\'router flagservice egress\\'']\n",
      "    ğŸ“Š Pattern counts: [1869]\n",
      "    â„¹ï¸ Found spanName pattern: 'router flagservice egress' (count: 1869)\n",
      "    ğŸ“Š No serviceName patterns found - attempting service inference from spanName patterns\n",
      "    ğŸ“Š Service inference from spans: {'ad': 1869}\n",
      "\n",
      "ğŸ¯ è¯†åˆ«å‡ºçš„serviceæ¨¡å¼:\n",
      "  - ad: 1869 æ¬¡æ¨¡å¼åŒ¹é…\n",
      "\n",
      "ğŸ’¡ åœ¨æ¨¡å¼ä¸­å‡ºç°æœ€å¤šçš„serviceName: ad\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Step 2: Pattern analysis with diff_patterns...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if top_95_percent_spans:\n",
    "    # é¦–å…ˆå°†å…¨éƒ¨é«˜ç‹¬å æ—¶é—´çš„span_idæ‹¼æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œç”¨äºdiff_patternsæŸ¥è¯¢æ¡ä»¶\n",
    "    span_conditions_for_patterns = \" or \".join([f\"spanId='{span_id}'\" for span_id in top_95_percent_spans[:2000]])  # Limit for query size\n",
    "    \n",
    "    param_str = \"\"\"{\"minimum_support_fraction\": 0.03}\"\"\"\n",
    "    # æ ¸å¿ƒä¸º diff_patterns ç®—æ³•è°ƒç”¨ï¼Œè¿›è¡Œæ¨¡å¼å·®å¼‚åˆ†æ\n",
    "    diff_patterns_query = f\"\"\"\n",
    "duration > {DURATION_THRESHOLD} | set session enable_remote_functions=true; set session velox_support_row_constructor_enabled=true; \n",
    "with t0 as (\n",
    "    select spanName, serviceName, cast(duration as double) as duration,\n",
    "           JSON_EXTRACT_SCALAR(resources, '$[\"k8s.pod.ip\"]') AS pod_ip,\n",
    "           JSON_EXTRACT_SCALAR(resources, '$[\"k8s.node.name\"]') AS node_name,\n",
    "           JSON_EXTRACT_SCALAR(resources, '$[\"service.version\"]') AS service_version,  \n",
    "           if(({span_conditions_for_patterns}), 'true', 'false') as anomaly_label, \n",
    "           cast(if((statusCode = 2 or statusCode = 3), 1, 0) as double) as error_count \n",
    "    from log\n",
    "), \n",
    "t1 as (\n",
    "    select array_agg(spanName) as spanName, \n",
    "           array_agg(serviceName) as serviceName, \n",
    "           array_agg(duration) as duration,\n",
    "           array_agg(pod_ip) as pod_ip, \n",
    "           array_agg(node_name) as node_name, \n",
    "           array_agg(service_version) as service_version, \n",
    "           array_agg(anomaly_label) as anomaly_label, \n",
    "           array_agg(error_count) as error_count \n",
    "    from t0\n",
    "),\n",
    "t2 as (\n",
    "    select row(spanName, serviceName, anomaly_label) as table_row \n",
    "    from t1\n",
    "),\n",
    "t3 as (\n",
    "    select diff_patterns(table_row, ARRAY['spanName', 'serviceName', 'anomaly_label'], 'anomaly_label', 'true', 'false', '', '', '{param_str}') as ret \n",
    "    from t2\n",
    ")\n",
    "select * from t3\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"ğŸ“‹ å·²ç”Ÿæˆç”¨äºæ¨¡å¼åˆ†æçš„ diff_patterns æŸ¥è¯¢è¯­å¥\")\n",
    "    # print(f\"ğŸ¯ æ­£åœ¨åˆ†æå‰ {min(50, len(top_95_percent_spans))} ä¸ªé«˜ç‹¬å æ—¶é—´spançš„æ¨¡å¼\")\n",
    "    \n",
    "    # ä½¿ç”¨SLSå®¢æˆ·ç«¯è¿›è¡ŒæŸ¥è¯¢\n",
    "    print(\"\\nğŸš€ æ­£åœ¨ä½¿ç”¨SLSå®¢æˆ·ç«¯æ‰§è¡Œ diff_patterns æŸ¥è¯¢...\")\n",
    "    \n",
    "    try:\n",
    "        # åˆ›å»ºç”¨äº diff_patterns æŸ¥è¯¢çš„ GetLogsRequest\n",
    "        from aliyun.log import GetLogsRequest\n",
    "        \n",
    "        request = GetLogsRequest(\n",
    "            project=PROJECT_NAME,\n",
    "            logstore=LOGSTORE_NAME,\n",
    "            query=diff_patterns_query.strip(),\n",
    "            fromTime=int(time.mktime(datetime.strptime(ANOMALY_START_TIME, \"%Y-%m-%d %H:%M:%S\").timetuple())),\n",
    "            toTime=int(time.mktime(datetime.strptime(ANOMALY_END_TIME, \"%Y-%m-%d %H:%M:%S\").timetuple())),\n",
    "            line=100  # Limit results\n",
    "        )\n",
    "        \n",
    "        # ä½¿ç”¨finderçš„SLSå®¢æˆ·ç«¯æ‰§è¡ŒæŸ¥è¯¢\n",
    "        patterns_result = finder.client.get_logs(request)\n",
    "        \n",
    "        if patterns_result and patterns_result.get_logs():\n",
    "            logs = [log_item.get_contents() for log_item in patterns_result.get_logs()]\n",
    "            print(f\"âœ… æ¨¡å¼åˆ†æå®Œæˆï¼šå…±è¿”å› {len(logs)} æ¡ç»“æœè®°å½•\")\n",
    "            \n",
    "            # å±•ç¤ºæ¨¡å‹åˆ†æç»“æœ\n",
    "            # åœ¨ç»“æœä¸­ä¼šå±•ç¤ºåŒä¸€serviceåœ¨æ­£å¸¸æ ·æœ¬å’Œå¼‚å¸¸æ ·æœ¬ä¸­å‡ºç°çš„æ¬¡æ•°å¯¹æ¯”\n",
    "            print(\"\\nğŸ“Š æ¨¡å¼åˆ†æç»“æœ:\")\n",
    "            for i, log_entry in enumerate(logs[:3]):  # Show first 3 results\n",
    "                print(f\"  ç»“æœ {i+1}: {log_entry}\")\n",
    "                \n",
    "            # Extract service patterns from diff_patterns results\n",
    "            service_patterns = {}\n",
    "            span_patterns = []  # Store span patterns for service inference\n",
    "            \n",
    "            for log_entry in logs:\n",
    "                if hasattr(log_entry, 'get_contents'):\n",
    "                    contents = log_entry.get_contents()\n",
    "                else:\n",
    "                    contents = log_entry\n",
    "                print(f\"  ğŸ” Analyzing pattern result: {contents}\")\n",
    "                \n",
    "                # Parse structured result from diff_patterns query\n",
    "                if 'ret' in contents:\n",
    "                    ret_value = contents['ret']\n",
    "                    \n",
    "                    if isinstance(ret_value, str):\n",
    "                        try:\n",
    "                            # Replace 'null' with 'None' for Python parsing\n",
    "                            data_str = ret_value.replace('null', 'None')\n",
    "                            result = eval(data_str)\n",
    "                            \n",
    "                            if len(result) >= 2 and isinstance(result[0], list) and isinstance(result[1], list):\n",
    "                                patterns = result[0]  # Pattern names\n",
    "                                counts = result[1]    # Pattern counts\n",
    "                                \n",
    "                                print(f\"    ğŸ“Š Extracted patterns: {patterns}\")\n",
    "                                print(f\"    ğŸ“Š Pattern counts: {counts}\")\n",
    "                                \n",
    "                                for i, pattern in enumerate(patterns):\n",
    "                                    if i < len(counts):\n",
    "                                        count = counts[i]\n",
    "                                        \n",
    "                                        # Parse serviceName patterns from diff_patterns results\n",
    "                                        if 'serviceName' in pattern and '=' in pattern:\n",
    "                                            # Handle complex patterns like \"serviceName\"='cart' AND \"spanName\"='POST'\n",
    "                                            import re\n",
    "                                            match = re.search(r'\"serviceName\"=\\'([^\\']+)\\'', pattern)\n",
    "                                            if match:\n",
    "                                                service_part = match.group(1)\n",
    "                                                service_patterns[service_part] = service_patterns.get(service_part, 0) + count\n",
    "                                                print(f\"    âœ… Found serviceName pattern: '{service_part}' (count: {count})\")\n",
    "                                            else:\n",
    "                                                print(f\"    âš ï¸ Could not parse serviceName from: '{pattern}'\")\n",
    "                                        \n",
    "                                        # Log spanName patterns for service inference\n",
    "                                        elif 'spanName' in pattern:\n",
    "                                            span_name = pattern.split('=')[1].strip('\\'\"') if '=' in pattern else pattern\n",
    "                                            print(f\"    â„¹ï¸ Found spanName pattern: '{span_name}' (count: {count})\")\n",
    "                                            span_patterns.append((span_name, count))\n",
    "                                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"    âš ï¸ Error parsing ret field: {e}\")\n",
    "                            import traceback\n",
    "                            traceback.print_exc()\n",
    "                \n",
    "            # If no serviceName patterns found, try to infer from spanName patterns\n",
    "            if not service_patterns and span_patterns:\n",
    "                print(f\"    ğŸ“Š No serviceName patterns found - attempting service inference from spanName patterns\")\n",
    "                \n",
    "                service_candidates = {}\n",
    "                for span_name, count in span_patterns:\n",
    "                    # Map spanName patterns to likely services\n",
    "                    if 'CartService' in span_name or 'cart' in span_name.lower():\n",
    "                        service_candidates['cart'] = service_candidates.get('cart', 0) + count\n",
    "                    elif 'ProductCatalogService' in span_name or 'product' in span_name.lower():\n",
    "                        service_candidates['product-catalog'] = service_candidates.get('product-catalog', 0) + count  \n",
    "                    elif 'PaymentService' in span_name or 'payment' in span_name.lower():\n",
    "                        service_candidates['payment'] = service_candidates.get('payment', 0) + count\n",
    "                    elif 'CheckoutService' in span_name or 'checkout' in span_name.lower():\n",
    "                        service_candidates['checkout'] = service_candidates.get('checkout', 0) + count\n",
    "                    elif 'RecommendationService' in span_name or 'recommendation' in span_name.lower():\n",
    "                        service_candidates['recommendation'] = service_candidates.get('recommendation', 0) + count\n",
    "                    elif 'CurrencyService' in span_name or 'currency' in span_name.lower():\n",
    "                        service_candidates['currency'] = service_candidates.get('currency', 0) + count\n",
    "                    elif 'frontend' in span_name.lower():\n",
    "                        service_candidates['frontend'] = service_candidates.get('frontend', 0) + count\n",
    "                    elif 'flagservice' in span_name.lower() or 'ad' in span_name.lower():\n",
    "                        service_candidates['ad'] = service_candidates.get('ad', 0) + count\n",
    "                    else:\n",
    "                        print(f\"    â“ Cannot infer service from span: '{span_name}'\")\n",
    "                \n",
    "                if service_candidates:\n",
    "                    print(f\"    ğŸ“Š Service inference from spans: {dict(service_candidates)}\")\n",
    "                    service_patterns = service_candidates\n",
    "                else:\n",
    "                    print(f\"    âŒ Cannot determine target service from available span patterns\")\n",
    "                    \n",
    "            # Store span patterns globally for additional analysis if needed\n",
    "            globals()['span_patterns'] = span_patterns\n",
    "                    \n",
    "            if service_patterns:\n",
    "                print(f\"\\nğŸ¯ è¯†åˆ«å‡ºçš„serviceæ¨¡å¼:\")\n",
    "                for service, count in sorted(service_patterns.items(), key=lambda x: x[1], reverse=True):\n",
    "                    print(f\"  - {service}: {count} æ¬¡æ¨¡å¼åŒ¹é…\")\n",
    "                    \n",
    "                # æ ¹æ®æœ€å¸¸è§æ¨¡å¼æ›´æ–°TARGET_SERVICE\n",
    "                most_common_service = max(service_patterns.items(), key=lambda x: x[1])[0]\n",
    "                print(f\"\\nğŸ’¡ åœ¨æ¨¡å¼ä¸­å‡ºç°æœ€å¤šçš„serviceName: {most_common_service}\")\n",
    "                \n",
    "                # å­˜å‚¨å‡ºç°æœ€é¢‘ç¹çš„service nameï¼Œä¾›åç»­æ­¥éª¤ä½¿ç”¨\n",
    "                globals()['TARGET_SERVICE_FROM_PATTERNS'] = most_common_service\n",
    "                \n",
    "        else:\n",
    "            print(\"âš ï¸  æœªè¿”å›ä»»ä½•æ¨¡å¼åˆ†æç»“æœ\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ‰§è¡Œ diff_patterns æŸ¥è¯¢æ—¶å‡ºé”™: {e}\")\n",
    "        print(\"ğŸ’¡ å»ºè®®åœ¨SLSæ§åˆ¶å°æ‰‹åŠ¨æ‰§è¡Œ\")\n",
    "    \n",
    "        # æ‰“å°æŸ¥è¯¢è¯­å¥ç”¨äºSLSæ§åˆ¶å°æ‰‹åŠ¨æ‰§è¡Œï¼ˆå¦‚æœ‰éœ€è¦ï¼‰\n",
    "        print(\"\\nğŸ“ SLSæ§åˆ¶å°æ‰‹åŠ¨æ‰§è¡Œç”¨æŸ¥è¯¢è¯­å¥ï¼ˆå¦‚æœ‰éœ€è¦ï¼‰ï¼š\")\n",
    "        print(\"=\"*50)\n",
    "        print(diff_patterns_query)\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(\"\\nğŸ’¡ é¢„æœŸç»“æœï¼šè¯¥æŸ¥è¯¢åº”è¯¥èƒ½è¯†åˆ«å“ªä¸ª serviceName æ‹¥æœ‰æœ€å¤šé«˜ç‹¬å æ—¶é—´çš„span\")\n",
    "        print(\"   ä¾‹å¦‚ï¼šåˆ†æç»“æœå¯èƒ½æ˜¾ç¤º serviceName='recommendation' çš„spanå æ¯”æœ€é«˜\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  æ— æ³•è¿›è¡Œæ¨¡å¼åˆ†æ - æœªæ‰¾åˆ°é«˜ç‹¬å æ—¶é—´çš„span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤3ï¼šCPUæŒ‡æ ‡åˆ†æ\n",
    "\n",
    "æ ¹æ®æ¨¡å¼åˆ†æç»“æœï¼ˆä¾‹å¦‚ï¼Œå¦‚æœ recommendation æœåŠ¡æ˜¾ç¤ºç‹¬å æ—¶é—´é«˜ï¼‰ï¼Œä½¿ç”¨CMSå®ä½“æŸ¥è¯¢ï¼Œå¯¹è¯¥serviceçš„CPUæŒ‡æ ‡è¿›è¡ŒæŸ¥è¯¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Step 3: CPU Metrics Analysis...\n",
      "============================================================\n",
      "âœ… æˆåŠŸè·å–ä¸´æ—¶è®¿é—®å‡­è¯ï¼\n",
      "âœ… å·²é€šè¿‡å¯¼å…¥çš„ TestCMSQuery åˆå§‹åŒ–CMSå®¢æˆ·ç«¯\n",
      "ğŸ”§ CMSå®¢æˆ·ç«¯å·²åˆå§‹åŒ–\n",
      "ğŸ”§ workspace: quanxi-tianchi-test\n",
      "ğŸ”§ Endpoint: cms.cn-qingdao.aliyuncs.com\n",
      "ğŸ¯ Using service from diff_patterns: ad\n",
      "ğŸ¯ æ­£åœ¨åˆ†æç›®æ ‡service:adçš„CPUæŒ‡æ ‡\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Step 3: CPU Metrics Analysis...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åˆå§‹åŒ–CMSæµ‹è¯•å®¢æˆ·ç«¯ï¼Œç”¨äºæŒ‡æ ‡æŸ¥è¯¢\n",
    "# å¦‚æœå­˜åœ¨å¯¼å…¥é—®é¢˜é€šè¿‡ç›´æ¥åˆ›å»ºç±»ä¿®å¤(except)\n",
    "try:\n",
    "    if TestCMSQuery is not None:\n",
    "        cms_tester = TestCMSQuery()\n",
    "        cms_tester.setUp()\n",
    "        print(f\"âœ… å·²é€šè¿‡å¯¼å…¥çš„ TestCMSQuery åˆå§‹åŒ–CMSå®¢æˆ·ç«¯\")\n",
    "    else:\n",
    "        raise ImportError(\"TestCMSQuery is None\")\n",
    "except:\n",
    "    print(\"âš ï¸  TestCMSQuery import failed, creating CMS client directly...\")\n",
    "    \n",
    "    import os\n",
    "    from alibabacloud_cms20240330.client import Client as Cms20240330Client\n",
    "    from alibabacloud_tea_openapi import models as open_api_models\n",
    "    from alibabacloud_cms20240330 import models as cms_20240330_models\n",
    "    from alibabacloud_tea_util import models as util_models\n",
    "    \n",
    "    \n",
    "    class DirectCMSClient:\n",
    "        def __init__(self):\n",
    "            self.access_key_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')\n",
    "            self.access_key_secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')\n",
    "            self.workspace = CMS_WORKSPACE\n",
    "            self.endpoint = CMS_ENDPOINT\n",
    "            \n",
    "            if not self.access_key_id or not self.access_key_secret:\n",
    "                raise ValueError(\"è¯·è®¾ç½®ç¯å¢ƒå˜é‡ ALIBABA_CLOUD_ACCESS_KEY_ID å’Œ ALIBABA_CLOUD_ACCESS_KEY_SECRET\")\n",
    "            \n",
    "            config = open_api_models.Config(\n",
    "                access_key_id=self.access_key_id,\n",
    "                access_key_secret=self.access_key_secret,\n",
    "            )\n",
    "            config.endpoint = self.endpoint\n",
    "            self.cms_client = Cms20240330Client(config)\n",
    "        \n",
    "        def _execute_spl_query(self, query: str, from_time: int = None, to_time: int = None):\n",
    "            \"\"\"æ‰§è¡ŒSPLæŸ¥è¯¢\"\"\"\n",
    "            if from_time is None:\n",
    "                from_time = int(time.time()) - 60 * 60 * 1\n",
    "            if to_time is None:\n",
    "                to_time = int(time.time())\n",
    "            \n",
    "            try:\n",
    "                headers = cms_20240330_models.GetEntityStoreDataHeaders()\n",
    "                request = cms_20240330_models.GetEntityStoreDataRequest(\n",
    "                    query=query,\n",
    "                    from_=from_time,\n",
    "                    to=to_time\n",
    "                )\n",
    "                runtime = util_models.RuntimeOptions()\n",
    "                response = self.cms_client.get_entity_store_data_with_options(\n",
    "                    self.workspace, request, headers, runtime\n",
    "                )\n",
    "                return response.body\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ CMSæŸ¥è¯¢é”™è¯¯: {e}\")\n",
    "                return None\n",
    "    \n",
    "    cms_tester = DirectCMSClient()\n",
    "    print(f\"âœ… CMS client created directly\")\n",
    "\n",
    "print(f\"ğŸ”§ CMSå®¢æˆ·ç«¯å·²åˆå§‹åŒ–\")\n",
    "print(f\"ğŸ”§ workspace: {CMS_WORKSPACE}\")\n",
    "print(f\"ğŸ”§ Endpoint: {CMS_ENDPOINT}\")\n",
    "\n",
    "# Determine target service from runtime observations\n",
    "def infer_service_from_additional_evidence():\n",
    "    \"\"\"Try additional service inference methods when primary analysis fails\"\"\"\n",
    "    if 'span_patterns' not in globals():\n",
    "        return None\n",
    "    \n",
    "    service_candidates = {}\n",
    "    for span_name, count in span_patterns:\n",
    "        # More comprehensive service mapping\n",
    "        if 'CartService' in span_name:\n",
    "            service_candidates['cart'] = service_candidates.get('cart', 0) + count\n",
    "        elif 'ProductCatalogService' in span_name:\n",
    "            service_candidates['product-catalog'] = service_candidates.get('product-catalog', 0) + count  \n",
    "        elif 'PaymentService' in span_name:\n",
    "            service_candidates['payment'] = service_candidates.get('payment', 0) + count\n",
    "        elif 'CheckoutService' in span_name:\n",
    "            service_candidates['checkout'] = service_candidates.get('checkout', 0) + count\n",
    "        elif 'RecommendationService' in span_name:\n",
    "            service_candidates['recommendation'] = service_candidates.get('recommendation', 0) + count\n",
    "        elif 'CurrencyService' in span_name:\n",
    "            service_candidates['currency'] = service_candidates.get('currency', 0) + count\n",
    "        elif 'frontend' in span_name.lower():\n",
    "            service_candidates['frontend'] = service_candidates.get('frontend', 0) + count\n",
    "        elif 'flagservice' in span_name.lower():\n",
    "            service_candidates['ad'] = service_candidates.get('ad', 0) + count\n",
    "    \n",
    "    if service_candidates:\n",
    "        best_service = max(service_candidates.items(), key=lambda x: x[1])\n",
    "        return best_service[0]\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Determine target service using runtime data\n",
    "if 'TARGET_SERVICE_FROM_PATTERNS' in globals():\n",
    "    TARGET_SERVICE = TARGET_SERVICE_FROM_PATTERNS \n",
    "    print(f\"ğŸ¯ Using service from diff_patterns: {TARGET_SERVICE}\")\n",
    "else:\n",
    "    # Try additional inference methods\n",
    "    inferred_service = infer_service_from_additional_evidence()\n",
    "    if inferred_service:\n",
    "        TARGET_SERVICE = inferred_service\n",
    "        print(f\"ğŸ¯ Inferred service from span patterns: {TARGET_SERVICE}\")\n",
    "    else:\n",
    "        print(f\"ğŸ¯ Cannot determine target service from available data\")\n",
    "        if 'span_patterns' in globals():\n",
    "            print(f\"    Available span patterns: {globals()['span_patterns']}\")\n",
    "        TARGET_SERVICE = None\n",
    "\n",
    "if TARGET_SERVICE:\n",
    "    print(f\"ğŸ¯ æ­£åœ¨åˆ†æç›®æ ‡service:{TARGET_SERVICE}çš„CPUæŒ‡æ ‡\")\n",
    "else:\n",
    "    print(f\"âš ï¸ æ— æ³•ç¡®å®šç›®æ ‡serviceï¼Œå°†è·³è¿‡serviceçº§åˆ«çš„åˆ†æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æ­£åœ¨æŸ¥è¯¢ç›®æ ‡serviceçš„CPUä½¿ç”¨ç‡ä¸é™åˆ¶...\n",
      "ğŸ” Query: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_vs_limits', 'range', '1m')\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_vs_limits', 'range', '1m')\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "âœ… å·²è·å– CPUä½¿ç”¨ç‡ä¸é™åˆ¶ æ•°æ®ï¼šå…± 1 æ¡è®°å½•\n",
      "ğŸ“‹ Fields: ['__labels__', '__name__', '__ts__', '__value__', '__source__']\n",
      "ğŸ“Š æ•°æ®æ ·ä¾‹ï¼ˆå‰3æ¡ï¼‰:\n",
      "  Record 1: ['{}', 'null', '[1756368270000000000,1756368330000000000,1756368390000000000,1756368450000000000,1756368510000000000,1756368570000000000,1756368630000000000,1756368690000000000,1756368750000000000,1756368810000000000,1756368870000000000,1756368930000000000,1756368990000000000,1756369050000000000,1756369110000000000,1756369170000000000]', '[84.80424496849061,84.70326216377088,48.483419603056947,48.550235283462047,48.745470612906299,49.63776885048156,48.97956500230081,46.312801670803128,83.14145336417026,40.15562929365869,41.73813178811215,41.81709069273207,83.61013319402048,84.48045858716316,84.39578008025194,84.14853695560683]', '']\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥è¯¢CPUä½¿ç”¨ç‡ä¸é™åˆ¶\n",
    "print(\"ğŸ“Š æ­£åœ¨æŸ¥è¯¢ç›®æ ‡serviceçš„CPUä½¿ç”¨ç‡ä¸é™åˆ¶...\")\n",
    "\n",
    "if TARGET_SERVICE:\n",
    "    cpu_usage_vs_limits_query = f\"\"\"\n",
    ".entity_set with(domain='k8s', name='k8s.deployment', query=`name='{TARGET_SERVICE}'` ) \n",
    "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_vs_limits', 'range', '1m')\n",
    "\"\"\"\n",
    "    print(f\"ğŸ” Query: {cpu_usage_vs_limits_query.strip()}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ è·³è¿‡CPUåˆ†æ - æ— ç›®æ ‡service\")\n",
    "    cpu_usage_vs_limits_query = None\n",
    "\n",
    "# æ‰§è¡ŒæŸ¥è¯¢\n",
    "if TARGET_SERVICE and cpu_usage_vs_limits_query:\n",
    "    try:\n",
    "        # å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºCMSæŸ¥è¯¢æ‰€éœ€æ—¶é—´æˆ³\n",
    "        from_time = int(time.mktime(datetime.strptime(NORMAL_START_TIME, \"%Y-%m-%d %H:%M:%S\").timetuple()))\n",
    "        to_time = int(time.mktime(datetime.strptime(ANOMALY_END_TIME, \"%Y-%m-%d %H:%M:%S\").timetuple()))\n",
    "        \n",
    "        cpu_usage_vs_limits_result = cms_tester._execute_spl_query(\n",
    "            cpu_usage_vs_limits_query.strip(),\n",
    "            from_time=from_time,\n",
    "            to_time=to_time\n",
    "        )\n",
    "        \n",
    "        if cpu_usage_vs_limits_result and cpu_usage_vs_limits_result.data:\n",
    "            print(f\"âœ… å·²è·å– CPUä½¿ç”¨ç‡ä¸é™åˆ¶ æ•°æ®ï¼šå…± {len(cpu_usage_vs_limits_result.data)} æ¡è®°å½•\")\n",
    "            \n",
    "            # å±•ç¤ºéƒ¨åˆ†æ•°æ®æ ·ä¾‹\n",
    "            if cpu_usage_vs_limits_result.header:\n",
    "                print(f\"ğŸ“‹ Fields: {cpu_usage_vs_limits_result.header}\")\n",
    "            \n",
    "            print(f\"ğŸ“Š æ•°æ®æ ·ä¾‹ï¼ˆå‰3æ¡ï¼‰:\")\n",
    "            for i, record in enumerate(cpu_usage_vs_limits_result.data[:3]):\n",
    "                print(f\"  Record {i+1}: {record}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  æœªæ‰¾åˆ° {TARGET_SERVICE} çš„ CPUä½¿ç”¨ç‡ä¸é™åˆ¶ æ•°æ®\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æŸ¥è¯¢ CPUä½¿ç”¨ç‡ä¸é™åˆ¶ æ—¶å‡ºé”™: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ è·³è¿‡CPUåˆ†æ - æ— æœ‰æ•ˆç›®æ ‡service\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š æ­£åœ¨æŸ¥è¯¢ç›®æ ‡serviceçš„å†…å­˜ä½¿ç”¨ç‡ä¸é™åˆ¶ï¼ˆç”¨äºå¯¹æ¯”ï¼‰...\n",
      "ğŸ” Query: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_vs_limits', 'range', '1m')\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_vs_limits', 'range', '1m')\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "âœ… å·²è·å–å†…å­˜ä½¿ç”¨ç‡ä¸é™åˆ¶æ•°æ®ï¼šå…±: 1 æ¡è®°å½•\n",
      "ğŸ“Š æ•°æ®æ ·ä¾‹ï¼ˆå‰3æ¡ï¼‰:\n",
      "  Record 1: ['{}', 'null', '[1756368270000000000,1756368330000000000,1756368390000000000,1756368450000000000,1756368510000000000,1756368570000000000,1756368630000000000,1756368690000000000,1756368750000000000,1756368810000000000,1756368870000000000,1756368930000000000,1756368990000000000,1756369050000000000,1756369110000000000,1756369170000000000]', '[63.032986111111117,63.10199652777778,39.310763888888889,48.71918402777778,55.824869791666667,28.9296875,42.41493055555556,49.654296875,52.95269097222223,37.02821180555556,48.14605034722222,56.29774305555556,61.69357638888889,60.81727430555556,61.55078125,61.588541666666667]', '']\n"
     ]
    }
   ],
   "source": [
    "# æŸ¥è¯¢è¯¥serviceçš„å†…å­˜ä½¿ç”¨ç‡ä¸é™åˆ¶ï¼Œä½œä¸ºå¯¹æ¯”\n",
    "print(\"ğŸ“Š æ­£åœ¨æŸ¥è¯¢ç›®æ ‡serviceçš„å†…å­˜ä½¿ç”¨ç‡ä¸é™åˆ¶ï¼ˆç”¨äºå¯¹æ¯”ï¼‰...\")\n",
    "\n",
    "memory_usage_vs_limits_query = f\"\"\"\n",
    ".entity_set with(domain='k8s', name='k8s.deployment', query=`name='{TARGET_SERVICE}'` ) \n",
    "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_vs_limits', 'range', '1m')\n",
    "\"\"\"\n",
    "\n",
    "print(f\"ğŸ” Query: {memory_usage_vs_limits_query.strip()}\")\n",
    "\n",
    "try:\n",
    "    memory_usage_vs_limits_result = cms_tester._execute_spl_query(\n",
    "        memory_usage_vs_limits_query.strip(),\n",
    "        from_time=from_time,\n",
    "        to_time=to_time\n",
    "    )\n",
    "    \n",
    "    if memory_usage_vs_limits_result and memory_usage_vs_limits_result.data:\n",
    "        print(f\"âœ… å·²è·å–å†…å­˜ä½¿ç”¨ç‡ä¸é™åˆ¶æ•°æ®ï¼šå…±: {len(memory_usage_vs_limits_result.data)} æ¡è®°å½•\")\n",
    "        \n",
    "        # å±•ç¤ºæ•°æ®æ ·ä¾‹\n",
    "        print(f\"ğŸ“Š æ•°æ®æ ·ä¾‹ï¼ˆå‰3æ¡ï¼‰:\")\n",
    "        for i, record in enumerate(memory_usage_vs_limits_result.data[:3]):\n",
    "            print(f\"  Record {i+1}: {record}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  æœªæ‰¾åˆ° {TARGET_SERVICE} çš„å†…å­˜ä½¿ç”¨ç‡ä¸é™åˆ¶æ•°æ®\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æŸ¥è¯¢å†…å­˜ä½¿ç”¨ç‡ä¸é™åˆ¶æ—¶å‡ºé”™: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤4ï¼šä½¿ç”¨ series_decompose_anomalies è¿›è¡Œå¼‚å¸¸æ£€æµ‹\n",
    "\n",
    "åˆ©ç”¨SPLå†…ç½®çš„å¼‚å¸¸æ£€æµ‹åŠŸèƒ½ï¼Œè‡ªåŠ¨è¯†åˆ«åœ¨å»¶è¿Ÿçªå¢æœŸé—´CPUä½¿ç”¨ç‡æŒ‡æ ‡æ˜¯å¦å‡ºç°å¼‚å¸¸è¡Œä¸ºã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Step 4: CPU Usage Anomaly Detection...\n",
      "============================================================\n",
      "ğŸ” Query: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "âœ… å·²è·å–å¼‚å¸¸æ£€æµ‹ç»“æœï¼šå…± 1 æ¡è®°å½•\n",
      "ğŸ“‹ Fields: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  Record 1: ['{}', 'null', '[1756368270000000000,1756368330000000000,1756368390000000000,1756368450000000000,1756368510000000000,1756368570000000000,1756368630000000000,1756368690000000000,1756368750000000000,1756368810000000000,1756368870000000000,1756368930000000000,1756368990000000000,1756369050000000000,1756369110000000000,1756369170000000000]', '[0.005228085438315369,0.005193791837076788,0.01574098264742213,0.05080104782465078,0.08491927941541674,0.1113010170296222,0.15188487392446407,0.17759752364160309,0.12284808320152696,0.15847613546839474,0.20648986170445578,0.21362204922587045,0.1278870787174181,0.1345670313771442,0.11598737606642884,0.06708615128261136]', '[[0.1092253029346466,0.10929421335458756,0.10935002565383911,0.1092720702290535,0.10951691120862961,0.10960525274276734,0.10960283130407334,0.10964372754096985,0.10960360616445542,0.10974538326263428,0.10979123413562775,0.10991497337818146,0.10995426774024964,0.11001035571098328,0.11001100391149521,0.1099604144692421],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null],null]', '[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]', '[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null]', 'null', '']\n",
      "ğŸš¨ åœ¨ ad çš„CPUä½¿ç”¨ç‡ä¸­æ£€æµ‹åˆ°å¼‚å¸¸ï¼\n",
      "    ğŸ“Š High anomaly score detected\n",
      "    ğŸ“Š High anomaly score detected\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Step 4: CPU Usage Anomaly Detection...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ä½¿ç”¨å¼‚å¸¸æ£€æµ‹è¿›è¡ŒæŸ¥è¯¢\n",
    "cpu_anomaly_detection_query = f\"\"\"\n",
    ".entity_set with(domain='k8s', name='k8s.deployment', query=`name='{TARGET_SERVICE}'` ) \n",
    "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
    "| extend ret = series_decompose_anomalies(__value__, '{{\"confidence\": 0.035}}')\n",
    "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
    "\"\"\"\n",
    "\n",
    "print(f\"ğŸ” Query: {cpu_anomaly_detection_query.strip()}\")\n",
    "\n",
    "try:\n",
    "    # å¢åŠ æ—¶é—´èŒƒå›´ä»¥è·å¾—æ›´å¥½çš„å¼‚å¸¸æ£€æµ‹ä¸Šä¸‹æ–‡\n",
    "    extended_from_time = from_time \n",
    "    extended_to_time = to_time \n",
    "    \n",
    "    anomaly_result = cms_tester._execute_spl_query(\n",
    "        cpu_anomaly_detection_query.strip(),\n",
    "        from_time=extended_from_time,\n",
    "        to_time=extended_to_time\n",
    "    )\n",
    "    \n",
    "    if anomaly_result and anomaly_result.data:\n",
    "        print(f\"âœ… å·²è·å–å¼‚å¸¸æ£€æµ‹ç»“æœï¼šå…± {len(anomaly_result.data)} æ¡è®°å½•\")\n",
    "        \n",
    "        if anomaly_result.header:\n",
    "            print(f\"ğŸ“‹ Fields: {anomaly_result.header}\")\n",
    "        \n",
    "        # åœ¨ç»“æœä¸­æŸ¥æ‰¾å¼‚å¸¸æŒ‡ç¤º\n",
    "        anomaly_found = False\n",
    "        anomaly_details = []\n",
    "        \n",
    "        for i, record in enumerate(anomaly_result.data[:5]):\n",
    "            print(f\"  Record {i+1}: {record}\")\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦æœ‰ ExceedUpperBound æˆ–å…¶ä»–å¼‚å¸¸æŒ‡ç¤º\n",
    "            if isinstance(record, (list, tuple)) and len(record) > 2:\n",
    "                # æŸ¥æ‰¾ anomalies_type_series å­—æ®µï¼ˆé€šå¸¸åœ¨å€’æ•°ç¬¬äºŒä½æˆ–æŒ‡å®šä½ç½®ï¼‰\n",
    "                for item in record:\n",
    "                    if isinstance(item, str):\n",
    "                        # æ£€æŸ¥ anomalies_type_series ä¸­æ˜¯å¦æœ‰ ExceedUpperBound\n",
    "                        if 'ExceedUpperBound' in item:\n",
    "                            anomaly_found = True\n",
    "                            anomaly_details.append('ExceedUpperBound detected')\n",
    "                        # æ£€æŸ¥å…¶ä»–å¼‚å¸¸æŒ‡ç¤º\n",
    "                        elif 'ExceedLowerBound' in item:\n",
    "                            anomaly_found = True\n",
    "                            anomaly_details.append('ExceedLowerBound detected')\n",
    "                        # æ£€æŸ¥å¼‚å¸¸åˆ†æ•°æ˜¯å¦å¤§äº0\n",
    "                        elif any(x in item for x in ['1.0', '1,0'] if ',' in item or '.' in item):\n",
    "                            # This indicates anomaly score\n",
    "                            if '1.0' in item or '1,0' in item:\n",
    "                                anomaly_found = True\n",
    "                                anomaly_details.append('High anomaly score detected')\n",
    "        \n",
    "        if anomaly_found:\n",
    "            print(f\"ğŸš¨ åœ¨ {TARGET_SERVICE} çš„CPUä½¿ç”¨ç‡ä¸­æ£€æµ‹åˆ°å¼‚å¸¸ï¼\")\n",
    "            for detail in anomaly_details:\n",
    "                print(f\"    ğŸ“Š {detail}\")\n",
    "        else:\n",
    "            print(f\"âœ… {TARGET_SERVICE} çš„CPUä½¿ç”¨ç‡æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âš ï¸  æœªæ‰¾åˆ° {TARGET_SERVICE} çš„å¼‚å¸¸æ£€æµ‹ç»“æœ\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ å¼‚å¸¸æ£€æµ‹è¿‡ç¨‹ä¸­å‡ºé”™: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥éª¤5ï¼šæ ¹å› åˆ†ææ€»ç»“\n",
    "\n",
    "æ€»ç»“åˆ†æç»“æœï¼Œå¹¶ç¡®å®šæœ€å¯èƒ½çš„æ ¹å› å€™é€‰é¡¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Step 5: Root Cause Analysis Summary\n",
      "============================================================\n",
      "ğŸ“Š åˆ†ææ€»ç»“ï¼š\n",
      "   å¼‚å¸¸æ—¶é—´æ®µï¼š2025-08-28 16:14:30 åˆ° 2025-08-28 16:19:30\n",
      "   æ­£å¸¸æ—¶é—´æ®µï¼š2025-08-28 16:04:30 åˆ° 2025-08-28 16:14:30\n",
      "   åˆ†æç›®æ ‡serviceï¼šad\n",
      "   å‘ç°é«˜ç‹¬å æ—¶é—´çš„spanæ•°é‡ï¼š1918\n",
      "\n",
      "ğŸ¯ æ ¹å› å‘ç°ï¼š\n",
      "   âœ… å·²è·å– ad çš„CPUä½¿ç”¨æ•°æ®\n",
      "   âœ… å·²è·å– ad çš„å†…å­˜ä½¿ç”¨æ•°æ®\n",
      "   âœ… å¼‚å¸¸æ£€æµ‹åˆ†æå·²å®Œæˆ\n",
      "   ğŸ“Š æ£€æµ‹åˆ°çš„å¼‚å¸¸ç‚¹æ€»æ•°ï¼š0\n",
      "   â„¹ï¸  å¼‚å¸¸æ£€æµ‹å·²å®Œæˆï¼Œä½†æœªå‘ç°å¼‚å¸¸\n",
      "\n",
      "ğŸ† æ ¹å› å€™é€‰ï¼š\n",
      "   ğŸ¯ ad.cpu\n",
      "   ğŸ“ˆ ç½®ä¿¡åº¦ï¼šä¸­\n",
      "   âŒ è¯æ®ï¼šFALSEï¼ˆæœªç¡®è®¤å¼‚å¸¸ï¼‰\n",
      "   ğŸ“ æ”¯æŒè¯æ®ï¼š\n",
      "      - å‘ç° 1918 ä¸ªé«˜ç‹¬å æ—¶é—´span\n",
      "      - æ¨¡å¼åˆ†æè¡¨æ˜æ¶‰åŠæœåŠ¡ ad\n",
      "      - æœ‰CPUæŒ‡æ ‡æ•°æ®ï¼Œä½†æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸\n",
      "\n",
      "ğŸ’¡ å»ºè®®ï¼š\n",
      "   - è¿›ä¸€æ­¥æ’æŸ¥ ad éƒ¨ç½²çš„èµ„æºä½¿ç”¨\n",
      "   - æ£€æŸ¥å¼‚å¸¸æœŸé—´ ad çš„æ—¥å¿—\n",
      "   - å³ä½¿æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸ï¼Œä¹Ÿè¦æ ¸æŸ¥æ˜¯å¦å­˜åœ¨ç»†å¾®æ€§èƒ½é—®é¢˜\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ æœ€ç»ˆç­”å¤ï¼šad.cpu\n",
      "ğŸ“ˆ ç½®ä¿¡åº¦ï¼šä¸­\n",
      "ğŸ” è¯æ®ï¼šFALSE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” Step 5: Root Cause Analysis Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åˆ†æç»“æœ\n",
    "print(f\"ğŸ“Š åˆ†ææ€»ç»“ï¼š\")\n",
    "print(f\"   å¼‚å¸¸æ—¶é—´æ®µï¼š{ANOMALY_START_TIME} åˆ° {ANOMALY_END_TIME}\")\n",
    "print(f\"   æ­£å¸¸æ—¶é—´æ®µï¼š{NORMAL_START_TIME} åˆ° {NORMAL_END_TIME}\")\n",
    "print(f\"   åˆ†æç›®æ ‡serviceï¼š{TARGET_SERVICE}\")\n",
    "print(f\"   å‘ç°é«˜ç‹¬å æ—¶é—´çš„spanæ•°é‡ï¼š{len(top_95_percent_spans) if 'top_95_percent_spans' in locals() else 0}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ æ ¹å› å‘ç°ï¼š\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦æœ‰CPUé—®é¢˜çš„è¯æ®\n",
    "cpu_evidence = False\n",
    "memory_evidence = False\n",
    "anomaly_evidence = False\n",
    "\n",
    "if 'cpu_usage_vs_limits_result' in locals() and cpu_usage_vs_limits_result and cpu_usage_vs_limits_result.data:\n",
    "    cpu_evidence = True\n",
    "    print(f\"   âœ… å·²è·å– {TARGET_SERVICE} çš„CPUä½¿ç”¨æ•°æ®\")\n",
    "else:\n",
    "    print(f\"   âŒ æœªæ‰¾åˆ° {TARGET_SERVICE} çš„CPUä½¿ç”¨æ•°æ®\")\n",
    "\n",
    "if 'memory_usage_vs_limits_result' in locals() and memory_usage_vs_limits_result and memory_usage_vs_limits_result.data:\n",
    "    memory_evidence = True\n",
    "    print(f\"   âœ… å·²è·å– {TARGET_SERVICE} çš„å†…å­˜ä½¿ç”¨æ•°æ®\")\n",
    "else:\n",
    "    print(f\"   âŒ æœªæ‰¾åˆ° {TARGET_SERVICE} çš„å†…å­˜ä½¿ç”¨æ•°æ®\")\n",
    "\n",
    "# æ£€æŸ¥å®é™…å¼‚å¸¸ç‚¹ï¼Œè‡³å°‘éœ€è¦3ä¸ªå¼‚å¸¸ç‚¹\n",
    "if 'anomaly_result' in locals() and anomaly_result and anomaly_result.data:\n",
    "    print(f\"   âœ… å¼‚å¸¸æ£€æµ‹åˆ†æå·²å®Œæˆ\")\n",
    "    \n",
    "    # ç»Ÿè®¡å¼‚å¸¸ç‚¹æ•°é‡ï¼Œè‡³å°‘3ä¸ªæ‰èƒ½ç¡®è®¤æœ‰å¼‚å¸¸\n",
    "    anomaly_point_count = 0\n",
    "    anomaly_types_found = []\n",
    "    \n",
    "    for record in anomaly_result.data:\n",
    "        if isinstance(record, (list, tuple)):\n",
    "            for item in record:\n",
    "                if isinstance(item, str):\n",
    "                    # ç»Ÿè®¡ExceedUpperBoundå‡ºç°æ¬¡æ•°\n",
    "                    exceed_upper_count = item.count('ExceedUpperBound')\n",
    "                    exceed_lower_count = item.count('ExceedLowerBound')\n",
    "                    \n",
    "                    anomaly_point_count += exceed_upper_count + exceed_lower_count\n",
    "                    \n",
    "                    if exceed_upper_count > 0:\n",
    "                        anomaly_types_found.extend(['ExceedUpperBound'] * exceed_upper_count)\n",
    "                    if exceed_lower_count > 0:\n",
    "                        anomaly_types_found.extend(['ExceedLowerBound'] * exceed_lower_count)\n",
    "    \n",
    "    print(f\"   ğŸ“Š æ£€æµ‹åˆ°çš„å¼‚å¸¸ç‚¹æ€»æ•°ï¼š{anomaly_point_count}\")\n",
    "    \n",
    "    # åªæœ‰å­˜åœ¨3ä¸ªåŠä»¥ä¸Šå¼‚å¸¸ç‚¹æ‰ç¡®è®¤å¼‚å¸¸\n",
    "    if anomaly_point_count >= 3:\n",
    "        anomaly_evidence = True\n",
    "        print(f\"   ğŸš¨ å¼‚å¸¸ç¡®è®¤ï¼šå‘ç° {anomaly_point_count} ä¸ªå¼‚å¸¸ç‚¹ï¼ˆå·²è¾¾â‰¥3é˜ˆå€¼ï¼‰\")\n",
    "        print(f\"   ğŸ“ å¼‚å¸¸ç±»å‹ï¼š{', '.join(set(anomaly_types_found))}\")\n",
    "    elif anomaly_point_count > 0:\n",
    "        anomaly_evidence = False\n",
    "        print(f\"   âš ï¸  å¼‚å¸¸è¯æ®ä¸è¶³ï¼šä»…æ£€æµ‹åˆ° {anomaly_point_count} ä¸ªç‚¹ï¼ˆéœ€è¦â‰¥3ï¼‰\")\n",
    "    else:\n",
    "        anomaly_evidence = False\n",
    "        print(f\"   â„¹ï¸  å¼‚å¸¸æ£€æµ‹å·²å®Œæˆï¼Œä½†æœªå‘ç°å¼‚å¸¸\")\n",
    "else:\n",
    "    print(f\"   âŒ å¼‚å¸¸æ£€æµ‹åˆ†æå¤±è´¥\")\n",
    "\n",
    "print(f\"\\nğŸ† æ ¹å› å€™é€‰ï¼š\")\n",
    "\n",
    "# åŸºäºevidenceçš„è¯„ä¼°ï¼šåªæœ‰å®é™…æ£€æµ‹åˆ°å¼‚å¸¸ç‚¹æ‰è®¾ç½® evidence=True\n",
    "evidence = anomaly_evidence and len(top_95_percent_spans) > 0\n",
    "\n",
    "if evidence and cpu_evidence:\n",
    "    root_cause_candidate = f\"{TARGET_SERVICE}.cpu\"\n",
    "    confidence = \"é«˜\"\n",
    "    \n",
    "    print(f\"   ğŸ¯ {root_cause_candidate}\")\n",
    "    print(f\"   ğŸ“ˆ ç½®ä¿¡åº¦ï¼š{confidence}\")\n",
    "    print(f\"   âœ… è¯æ®ï¼šTRUEï¼ˆå·²æ£€æµ‹åˆ°å¼‚å¸¸ï¼‰\")\n",
    "    print(f\"   ğŸ“ æ”¯æŒè¯æ®ï¼š\")\n",
    "    print(f\"      - å‘ç° {len(top_95_percent_spans)} ä¸ªé«˜ç‹¬å æ—¶é—´span\")\n",
    "    print(f\"      - æ¨¡å¼åˆ†æè¡¨æ˜æ¶‰åŠæœåŠ¡ {TARGET_SERVICE}\")\n",
    "    print(f\"      - æœ‰CPUæŒ‡æ ‡æ•°æ®å¯è¯¦ç»†åˆ†æ\")\n",
    "    print(f\"      - è‡ªåŠ¨å¼‚å¸¸æ£€æµ‹ç¡®è®¤äº†CPUä½¿ç”¨å¼‚å¸¸\")\n",
    "        \n",
    "elif len(top_95_percent_spans) > 0 and cpu_evidence:\n",
    "    root_cause_candidate = f\"{TARGET_SERVICE}.cpu\"\n",
    "    confidence = \"ä¸­\"\n",
    "    \n",
    "    print(f\"   ğŸ¯ {root_cause_candidate}\")\n",
    "    print(f\"   ğŸ“ˆ ç½®ä¿¡åº¦ï¼š{confidence}\")\n",
    "    print(f\"   âŒ è¯æ®ï¼šFALSEï¼ˆæœªç¡®è®¤å¼‚å¸¸ï¼‰\")\n",
    "    print(f\"   ğŸ“ æ”¯æŒè¯æ®ï¼š\")\n",
    "    print(f\"      - å‘ç° {len(top_95_percent_spans)} ä¸ªé«˜ç‹¬å æ—¶é—´span\")\n",
    "    print(f\"      - æ¨¡å¼åˆ†æè¡¨æ˜æ¶‰åŠæœåŠ¡ {TARGET_SERVICE}\")\n",
    "    print(f\"      - æœ‰CPUæŒ‡æ ‡æ•°æ®ï¼Œä½†æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸\")\n",
    "    \n",
    "else:\n",
    "    root_cause_candidate = \"unknown\"\n",
    "    confidence = \"ä½\"\n",
    "    \n",
    "    print(f\"   ğŸ¯ {root_cause_candidate}\")\n",
    "    print(f\"   ğŸ“ˆ ç½®ä¿¡åº¦ï¼š{confidence}\")\n",
    "    print(f\"   âŒ è¯æ®ï¼šFALSEï¼ˆè¯æ®ä¸è¶³ï¼‰\")\n",
    "    print(f\"   ğŸ“ æ”¯æŒè¯æ®ï¼š\")\n",
    "    print(f\"      - é«˜ç‹¬å æ—¶é—´spanæˆ–æŒ‡æ ‡æ•°æ®æœ‰é™\")\n",
    "    print(f\"      - åˆ†æå¯èƒ½éœ€è¦ä¸åŒçš„å‚æ•°æˆ–æ—¶é—´èŒƒå›´\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ å»ºè®®ï¼š\")\n",
    "if evidence:\n",
    "    print(f\"   - æ£€æŸ¥ {TARGET_SERVICE} éƒ¨ç½²çš„CPUèµ„æºé™åˆ¶\")\n",
    "    print(f\"   - æ£€æŸ¥å¼‚å¸¸æœŸé—´æ˜¯å¦æœ‰é«˜CPUæ¶ˆè€—æ“ä½œ\")\n",
    "    print(f\"   - è€ƒè™‘æ‰©å®¹ {TARGET_SERVICE} æˆ–ä¼˜åŒ–CPUä½¿ç”¨\")\n",
    "elif confidence == \"ä¸­\":\n",
    "    print(f\"   - è¿›ä¸€æ­¥æ’æŸ¥ {TARGET_SERVICE} éƒ¨ç½²çš„èµ„æºä½¿ç”¨\")\n",
    "    print(f\"   - æ£€æŸ¥å¼‚å¸¸æœŸé—´ {TARGET_SERVICE} çš„æ—¥å¿—\")\n",
    "    print(f\"   - å³ä½¿æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸ï¼Œä¹Ÿè¦æ ¸æŸ¥æ˜¯å¦å­˜åœ¨ç»†å¾®æ€§èƒ½é—®é¢˜\")\n",
    "else:\n",
    "    print(f\"   - è°ƒæ•´åˆ†æå‚æ•°ï¼ˆæ—¶é—´èŒƒå›´ã€æŒç»­æ—¶é—´é˜ˆå€¼ç­‰ï¼‰\")\n",
    "    print(f\"   - æ ¸æŸ¥æŒ‡å®šæ—¶é—´æ®µå†…æ•°æ®æ˜¯å¦é½å…¨\")\n",
    "    print(f\"   - å¯è€ƒè™‘æ‰©å±•åˆ°å…¶ä»–æœåŠ¡çš„åˆ†æ\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ¯ æœ€ç»ˆç­”å¤ï¼š{root_cause_candidate}\")\n",
    "print(f\"ğŸ“ˆ ç½®ä¿¡åº¦ï¼š{confidence}\")\n",
    "print(f\"ğŸ” è¯æ®ï¼š{'TRUE' if evidence else 'FALSE'}\")\n",
    "print(f\"\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é™„åŠ åˆ†æå·¥å…·\n",
    "\n",
    "ä»¥ä¸‹ä»£ç æä¾›äº†å¯æŒ‰éœ€è¿è¡Œçš„é™„åŠ åˆ†æåŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” é™„åŠ åˆ†æï¼šå…¶ä»–serviceçš„CPUå’Œå†…å­˜å¼‚å¸¸æ£€æµ‹\n",
      "============================================================\n",
      "\n",
      "ğŸ” æ­£åœ¨åˆ†æ frontend æœåŠ¡çš„å¼‚å¸¸...\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='frontend'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… frontendï¼šå·²è·å–CPUæŒ‡æ ‡æ•°æ®\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='frontend'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… frontendï¼šå·²è·å–å†…å­˜æŒ‡æ ‡æ•°æ®\n",
      "   âœ… frontendï¼šæœªæ£€æµ‹åˆ°å¼‚å¸¸\n",
      "\n",
      "ğŸ” æ­£åœ¨åˆ†æ cart æœåŠ¡çš„å¼‚å¸¸...\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='cart'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… cartï¼šå·²è·å–CPUæŒ‡æ ‡æ•°æ®\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='cart'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… cartï¼šå·²è·å–å†…å­˜æŒ‡æ ‡æ•°æ®\n",
      "   âœ… cartï¼šæœªæ£€æµ‹åˆ°å¼‚å¸¸\n",
      "\n",
      "ğŸ” æ­£åœ¨åˆ†æ checkout æœåŠ¡çš„å¼‚å¸¸...\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='checkout'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… checkoutï¼šå·²è·å–CPUæŒ‡æ ‡æ•°æ®\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='checkout'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… checkoutï¼šå·²è·å–å†…å­˜æŒ‡æ ‡æ•°æ®\n",
      "   âœ… checkoutï¼šæœªæ£€æµ‹åˆ°å¼‚å¸¸\n",
      "\n",
      "ğŸ” æ­£åœ¨åˆ†æ payment æœåŠ¡çš„å¼‚å¸¸...\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='payment'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… paymentï¼šå·²è·å–CPUæŒ‡æ ‡æ•°æ®\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='payment'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… paymentï¼šå·²è·å–å†…å­˜æŒ‡æ ‡æ•°æ®\n",
      "   âœ… paymentï¼šæœªæ£€æµ‹åˆ°å¼‚å¸¸\n",
      "\n",
      "ğŸ” æ­£åœ¨åˆ†æ shipping æœåŠ¡çš„å¼‚å¸¸...\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='shipping'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… shippingï¼šå·²è·å–CPUæŒ‡æ ‡æ•°æ®\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='shipping'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… shippingï¼šå·²è·å–å†…å­˜æŒ‡æ ‡æ•°æ®\n",
      "   âœ… shippingï¼šæœªæ£€æµ‹åˆ°å¼‚å¸¸\n",
      "\n",
      "ğŸ” æ­£åœ¨åˆ†æ currency æœåŠ¡çš„å¼‚å¸¸...\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='currency'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… currencyï¼šå·²è·å–CPUæŒ‡æ ‡æ•°æ®\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='currency'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… currencyï¼šå·²è·å–å†…å­˜æŒ‡æ ‡æ•°æ®\n",
      "   âœ… currencyï¼šæœªæ£€æµ‹åˆ°å¼‚å¸¸\n",
      "\n",
      "ğŸ” æ­£åœ¨åˆ†æ ad æœåŠ¡çš„å¼‚å¸¸...\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… adï¼šå·²è·å–CPUæŒ‡æ ‡æ•°æ®\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='ad'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… adï¼šå·²è·å–å†…å­˜æŒ‡æ ‡æ•°æ®\n",
      "   âœ… adï¼šæœªæ£€æµ‹åˆ°å¼‚å¸¸\n",
      "\n",
      "ğŸ” æ­£åœ¨åˆ†æ recommendation æœåŠ¡çš„å¼‚å¸¸...\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='recommendation'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… recommendationï¼šå·²è·å–CPUæŒ‡æ ‡æ•°æ®\n",
      "ğŸ” æŸ¥è¯¢å‚æ•°:\n",
      "  Workspace: quanxi-tianchi-test\n",
      "  æ—¶é—´èŒƒå›´: 2025-08-28 16:04:30 åˆ° 2025-08-28 16:19:30\n",
      "  æŸ¥è¯¢è¯­å¥: .entity_set with(domain='k8s', name='k8s.deployment', query=`name='recommendation'` ) \n",
      "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
      "| extend ret = series_decompose_anomalies(__value__, '{\"confidence\": 0.035}')\n",
      "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
      "\n",
      "ğŸ“Š æŸ¥è¯¢å“åº”:\n",
      "  çŠ¶æ€ç : 200\n",
      "  è¿”å›header: ['__labels__', '__name__', '__ts__', '__value__', 'ret', 'anomalies_score_series', 'anomalies_type_series', 'error_msg', '__source__']\n",
      "  è¿”å›dataè¡Œæ•°: 1\n",
      "\n",
      "   âœ… recommendationï¼šå·²è·å–å†…å­˜æŒ‡æ ‡æ•°æ®\n",
      "   âœ… recommendationï¼šæœªæ£€æµ‹åˆ°å¼‚å¸¸\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ å…¶ä»–æœåŠ¡å¼‚å¸¸æ±‡æ€»ï¼š\n",
      "âœ… å…¶ä»–æœåŠ¡æœªæ£€æµ‹åˆ°å¼‚å¸¸\n",
      "ğŸ’¡ ä¸»è¦å¼‚å¸¸ä¼¼ä¹åªå½±å“ ad æœåŠ¡\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Additionalï¼šå…¶ä»–serviceçš„é«˜çº§å¼‚å¸¸æ£€æµ‹\n",
    "print(\"ğŸ” é™„åŠ åˆ†æï¼šå…¶ä»–serviceçš„CPUå’Œå†…å­˜å¼‚å¸¸æ£€æµ‹\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# éœ€è¦æ£€æµ‹çš„å…¶ä»–å¸¸è§æœåŠ¡åˆ—è¡¨\n",
    "OTHER_SERVICES = [\"frontend\", \"cart\", \"checkout\", \"payment\", \"shipping\", \"currency\", \"ad\", \"recommendation\"]\n",
    "other_services_anomalies = {}\n",
    "\n",
    "for service in OTHER_SERVICES:\n",
    "    try:\n",
    "        print(f\"\\nğŸ” æ­£åœ¨åˆ†æ {service} æœåŠ¡çš„å¼‚å¸¸...\")\n",
    "\n",
    "        # CPUå¼‚å¸¸æ£€æµ‹\n",
    "        service_cpu_anomaly_query = f\"\"\"\n",
    ".entity_set with(domain='k8s', name='k8s.deployment', query=`name='{service}'` ) \n",
    "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_cpu_usage_total', 'range', '1m')\n",
    "| extend ret = series_decompose_anomalies(__value__, '{{\"confidence\": 0.035}}')\n",
    "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
    "\"\"\"\n",
    "        \n",
    "        cpu_result = cms_tester._execute_spl_query(\n",
    "            service_cpu_anomaly_query.strip(),\n",
    "            from_time=from_time,\n",
    "            to_time=to_time\n",
    "        )\n",
    "        \n",
    "        cpu_anomaly_found = False\n",
    "        cpu_anomaly_details = []\n",
    "        cpu_anomaly_count = 0\n",
    "        \n",
    "        if cpu_result and cpu_result.data:\n",
    "            print(f\"   âœ… {service}ï¼šå·²è·å–CPUæŒ‡æ ‡æ•°æ®\")\n",
    "            \n",
    "            # åœ¨CPUæ•°æ®ä¸­ç»Ÿè®¡å¼‚å¸¸ç‚¹æ•° - è‡³å°‘3ä¸ªæ‰ç®—å¼‚å¸¸\n",
    "            for record in cpu_result.data:\n",
    "                if isinstance(record, (list, tuple)):\n",
    "                    for item in record:\n",
    "                        if isinstance(item, str):\n",
    "                            exceed_upper_count = item.count('ExceedUpperBound')\n",
    "                            exceed_lower_count = item.count('ExceedLowerBound')\n",
    "                            cpu_anomaly_count += exceed_upper_count + exceed_lower_count\n",
    "            \n",
    "            # åªæœ‰åœ¨å¼‚å¸¸ç‚¹æ•°è¾¾åˆ°3åŠä»¥ä¸Šæ—¶æ‰è®¤ä¸ºæœ‰å¼‚å¸¸\n",
    "            if cpu_anomaly_count >= 3:\n",
    "                cpu_anomaly_found = True\n",
    "                cpu_anomaly_details.append(f'CPUå¼‚å¸¸ç‚¹ï¼š{cpu_anomaly_count} ä¸ª')\n",
    "        \n",
    "        # å†…å­˜å¼‚å¸¸æ£€æµ‹\n",
    "        service_memory_anomaly_query = f\"\"\"\n",
    ".entity_set with(domain='k8s', name='k8s.deployment', query=`name='{service}'` ) \n",
    "| entity-call get_metric('k8s', 'k8s.metric.high_level_metric_deployment', 'deployment_memory_usage_total', 'range', '1m')\n",
    "| extend ret = series_decompose_anomalies(__value__, '{{\"confidence\": 0.035}}')\n",
    "| extend anomalies_score_series = ret.anomalies_score_series, anomalies_type_series = ret.anomalies_type_series, error_msg = ret.error_msg\n",
    "\"\"\"\n",
    "        \n",
    "        memory_result = cms_tester._execute_spl_query(\n",
    "            service_memory_anomaly_query.strip(),\n",
    "            from_time=from_time,\n",
    "            to_time=to_time\n",
    "        )\n",
    "        \n",
    "        memory_anomaly_found = False\n",
    "        memory_anomaly_details = []\n",
    "        memory_anomaly_count = 0\n",
    "        \n",
    "        if memory_result and memory_result.data:\n",
    "            print(f\"   âœ… {service}ï¼šå·²è·å–å†…å­˜æŒ‡æ ‡æ•°æ®\")\n",
    "            \n",
    "            # åœ¨å†…å­˜æ•°æ®ä¸­ç»Ÿè®¡å¼‚å¸¸ç‚¹æ•° - è‡³å°‘3ä¸ªæ‰ç®—å¼‚å¸¸\n",
    "            for record in memory_result.data:\n",
    "                if isinstance(record, (list, tuple)):\n",
    "                    for item in record:\n",
    "                        if isinstance(item, str):\n",
    "                            exceed_upper_count = item.count('ExceedUpperBound')\n",
    "                            exceed_lower_count = item.count('ExceedLowerBound')\n",
    "                            memory_anomaly_count += exceed_upper_count + exceed_lower_count\n",
    "            \n",
    "            # åªæœ‰åœ¨å¼‚å¸¸ç‚¹æ•°è¾¾åˆ°3åŠä»¥ä¸Šæ—¶æ‰è®¤ä¸ºæœ‰å¼‚å¸¸\n",
    "            if memory_anomaly_count >= 3:\n",
    "                memory_anomaly_found = True\n",
    "                memory_anomaly_details.append(f'å†…å­˜å¼‚å¸¸ç‚¹ï¼š{memory_anomaly_count} ä¸ª')\n",
    "        \n",
    "        # æ±‡æ€»å½“å‰æœåŠ¡çš„å¼‚å¸¸åˆ†æç»“æœ\n",
    "        service_anomalies = cpu_anomaly_details + memory_anomaly_details\n",
    "        \n",
    "        if service_anomalies:\n",
    "            print(f\"   ğŸš¨ {service}ï¼šæ£€æµ‹åˆ°å¼‚å¸¸\")\n",
    "            for anomaly in service_anomalies:\n",
    "                print(f\"      ğŸ“Š {anomaly}\")\n",
    "            other_services_anomalies[service] = service_anomalies\n",
    "        else:\n",
    "            if cpu_result and cpu_result.data or memory_result and memory_result.data:\n",
    "                print(f\"   âœ… {service}ï¼šæœªæ£€æµ‹åˆ°å¼‚å¸¸\")\n",
    "            else:\n",
    "                print(f\"   âŒ {service}ï¼šæ— æŒ‡æ ‡æ•°æ®å¯ç”¨\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ {service}ï¼šå‡ºé”™ - {e}\")\n",
    "        \n",
    "    # å¢åŠ å°å»¶è¿Ÿä»¥é¿å…APIè¢«è¯·æ±‚è¿‡å¿«\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# å…¶ä»–æœåŠ¡å¼‚å¸¸æ£€æµ‹ç»“æœæ€»ç»“\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ¯ å…¶ä»–æœåŠ¡å¼‚å¸¸æ±‡æ€»ï¼š\")\n",
    "\n",
    "if other_services_anomalies:\n",
    "    print(f\"ğŸ“Š æ£€æµ‹åˆ°å¼‚å¸¸çš„æœåŠ¡ï¼š\")\n",
    "    for service, anomalies in other_services_anomalies.items():\n",
    "        print(f\"  ğŸš¨ {service}ï¼š\")\n",
    "        for anomaly in anomalies:\n",
    "            print(f\"    - {anomaly}\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ å»ºè®®è¿›ä¸€æ­¥è°ƒæŸ¥ï¼š\")\n",
    "    for service in other_services_anomalies.keys():\n",
    "        print(f\"   - æ£€æŸ¥ {service} éƒ¨ç½²çš„èµ„æºçº¦æŸæƒ…å†µ\")\n",
    "        print(f\"   - æ£€æŸ¥å¼‚å¸¸æœŸé—´ {service} çš„æ—¥å¿—\")\n",
    "        \n",
    "else:\n",
    "    print(f\"âœ… å…¶ä»–æœåŠ¡æœªæ£€æµ‹åˆ°å¼‚å¸¸\")\n",
    "    print(f\"ğŸ’¡ ä¸»è¦å¼‚å¸¸ä¼¼ä¹åªå½±å“ {TARGET_SERVICE} æœåŠ¡\")\n",
    "\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨è¯´æ˜\n",
    "\n",
    "1. **é…ç½®å‚æ•°** - åœ¨ç¬¬äºŒä¸ªå•å…ƒæ ¼ä¸­æ ¹æ®æ‚¨çš„å…·ä½“æ•…éšœäº‹ä»¶é…ç½®å‚æ•°\n",
    "2. **æŒ‰é¡ºåºè¿è¡Œæ‰€æœ‰å•å…ƒæ ¼** - åˆ†ææ­¥éª¤ç›¸äº’ä¾èµ–ï¼Œéœ€æŒ‰é¡ºåºæ‰§è¡Œ\n",
    "3. **æ£€æŸ¥æ¨¡å¼åˆ†ææŸ¥è¯¢** - åœ¨æ­¥éª¤2ä¸­æŸ¥çœ‹æ¨¡å¼åˆ†ææŸ¥è¯¢ï¼Œå¦‚éœ€è¦å¯åœ¨SLSæ§åˆ¶å°æ‰‹åŠ¨æ‰§è¡Œ\n",
    "4. **è°ƒæ•´ç›®æ ‡æœåŠ¡** - åœ¨æ­¥éª¤3ä¸­æ ¹æ®æ¨¡å¼åˆ†æç»“æœè°ƒæ•´ `TARGET_SERVICE`\n",
    "5. **æŸ¥çœ‹æœ€ç»ˆæ€»ç»“** - åœ¨æ­¥éª¤5ä¸­æŸ¥çœ‹æ ¹å› å€™é€‰çš„æœ€ç»ˆæ€»ç»“\n",
    "\n",
    "### é¢„æœŸå·¥ä½œæµç»“æœ\n",
    "\n",
    "- **æ­¥éª¤1**: è¯†åˆ«å…·æœ‰å¼‚å¸¸é«˜ç‹¬å æ—¶é—´çš„span\n",
    "- **æ­¥éª¤2**: æ­ç¤ºå“ªä¸ªæœåŠ¡ï¼ˆå¦‚ `recommendation`ï¼‰å­˜åœ¨æœ€å¤šé—®é¢˜span\n",
    "- **æ­¥éª¤3**: æ˜¾ç¤ºå·²è¯†åˆ«æœåŠ¡çš„CPUä½¿ç”¨ç‡æŒ‡æ ‡\n",
    "- **æ­¥éª¤4**: ç¡®è®¤CPUä½¿ç”¨ç‡æ˜¯å¦æ˜¾ç¤ºå¼‚å¸¸æ¨¡å¼\n",
    "- **æ­¥éª¤5**: å¾—å‡ºæ ¹å› ç»“è®ºï¼ˆå¦‚ `recommendation.cpu`ï¼‰\n",
    "\n",
    "### æ•…éšœæ’é™¤\n",
    "\n",
    "- **æœªæ‰¾åˆ°é«˜ç‹¬å æ—¶é—´span**: è°ƒæ•´ `DURATION_THRESHOLD` æˆ–æ—¶é—´èŒƒå›´\n",
    "- **CMSæŸ¥è¯¢å¤±è´¥**: éªŒè¯ç¯å¢ƒå˜é‡å’Œå·¥ä½œç©ºé—´è®¿é—®æƒé™\n",
    "- **æ¨¡å¼åˆ†ææ˜¾ç¤ºæ²¡æœ‰æ˜ç¡®çš„æœåŠ¡**: æ£€æŸ¥æ˜¯å¦å¤šä¸ªæœåŠ¡å—å½±å“\n",
    "- **æœªæ£€æµ‹åˆ°å¼‚å¸¸**: é—®é¢˜å¯èƒ½ä¸CPUæ— å…³ï¼›æ£€æŸ¥å†…å­˜ã€ç½‘ç»œæˆ–å¤–éƒ¨ä¾èµ–"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
